{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA5xhvwsyAOt"
   },
   "source": [
    "# BERT Pipeline for Drug-Target Interaction (DTI) Prediction\n",
    "\n",
    "This notebook implements an end-to-end pipeline for predicting drug-target interactions using a BERT-based model. The workflow includes:\n",
    "\n",
    "- *Data Preprocessing:* Cleaning, merging, and filtering raw CSV datasets.\n",
    "- *Frequent Consecutive Subsequence (FCS) Extraction:* Extracting and ranking meaningful subsequences from drug SMILES and protein sequences.\n",
    "- *Sequence Encoding:* Converting sequences into token indices using generated token dictionaries.\n",
    "- *BERT Input Construction and Masking:* Creating unified BERT inputs with special tokens and applying dynamic masking.\n",
    "- *Model Training and Evaluation:* Training a BERT-based classifier and evaluating its performance using ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3080,
     "status": "ok",
     "timestamp": 1740736084777,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "JX_Cybg2wSqf",
    "outputId": "9283ddfe-3fb7-4e9c-81f5-54a92e0a58fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.11/site-packages (3.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUXxVB0ydYU"
   },
   "source": [
    "##Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "t5bw0HDDv8wQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "# from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm # For processing bars\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "KWnFuIFoLAUX"
   },
   "outputs": [],
   "source": [
    "#Define file paths\n",
    "DRUGBANK_CSV = \"drug_smiles.csv\"\n",
    "PROTEIN_CSV = \"pdb_sequences.csv\"\n",
    "INTERACTIONS_CSV = \"confirmed_interactions.csv\"\n",
    "DRUG_ENCODED_CSV = \"drug_encoded.csv\"\n",
    "PROTEIN_ENCODED_CSV = \"protein_encoded.csv\"\n",
    "DRUG_FCS_CSV = \"drug_smiles_fcs_freq_100.csv\"\n",
    "PROTEIN_FCS_CSV = \"protein_fcs_freq_100.csv\"\n",
    "MERGED_ENCODING_CSV = \"merged_encodings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBiF18kZyiCk"
   },
   "source": [
    "# Frequent Consecutive Subsequence (FCS) Extraction\n",
    "\n",
    "This section extracts all consecutive subsequences (using a sliding window) from the input strings (drug SMILES and protein sequences). The extraction function also prunes low-frequency subsequences periodically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "vUsfGha1v8wT"
   },
   "outputs": [],
   "source": [
    "def extract_fcs_subsequences_stream(strings, min_length=2, max_length=None,\n",
    "                                      prune_every=1000, prune_threshold=5,\n",
    "                                      preserve_short_length=5):\n",
    "    \"\"\"\n",
    "    Extract frequent consecutive subsequences from a list of strings in a streaming fashion.\n",
    "    Uses different pruning criteria: always keep subsequences shorter than `preserve_short_length`\n",
    "    while pruning longer ones based on prune_threshold.\n",
    "\n",
    "    :param strings: Iterable of strings (e.g., drug SMILES or protein sequences).\n",
    "    :param min_length: Minimum subsequence length.\n",
    "    :param max_length: Maximum subsequence length to consider (if None, use full length available).\n",
    "    :param prune_every: After processing this many sequences, prune the counter.\n",
    "    :param prune_threshold: For subsequences of length >= preserve_short_length,\n",
    "                            remove those with counts lower than this threshold during pruning.\n",
    "    :param preserve_short_length: Subsequences with length less than this value will be preserved regardless.\n",
    "    :return: A Counter mapping each subsequence to its frequency.\n",
    "    \"\"\"\n",
    "    subseq_counter = Counter()\n",
    "\n",
    "    for idx, s in enumerate(strings, 1):\n",
    "        n = len(s)\n",
    "        for i in range(n):\n",
    "            current_max = n - i if max_length is None else min(max_length, n - i)\n",
    "            for l in range(min_length, current_max + 1):\n",
    "                subseq = s[i:i+l]\n",
    "                subseq_counter[subseq] += 1\n",
    "\n",
    "        # Prune periodically to free memory:\n",
    "        if idx % prune_every == 0:\n",
    "            # For short subsequences (length < preserve_short_length), keep them always;\n",
    "            # for longer ones, only keep if count >= prune_threshold.\n",
    "            subseq_counter = Counter({\n",
    "                k: v for k, v in subseq_counter.items()\n",
    "                if len(k) < preserve_short_length or v >= prune_threshold\n",
    "            })\n",
    "            print(f\"Processed {idx} sequences, counter pruned to {len(subseq_counter)} keys.\")\n",
    "\n",
    "    return subseq_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTYbmKKwzJwF"
   },
   "source": [
    "# Filtering and Ranking Subsequences\n",
    "\n",
    "We filter the extracted subsequences by applying a minimum frequency threshold and then rank them. The result is saved into a DataFrame containing each subsequence, its frequency, and rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "rute13mov8wU"
   },
   "outputs": [],
   "source": [
    "def filter_and_rank_fcs(fcs_counts, min_frequency=10):\n",
    "    \"\"\"\n",
    "    Filter and rank subsequences that appear with frequency >= min_frequency.\n",
    "\n",
    "    :param fcs_counts: Counter mapping subsequences to frequency.\n",
    "    :param min_frequency: Minimum frequency for a subsequence to be included.\n",
    "    :return: A pandas DataFrame with columns [Subsequence, Frequency, Rank].\n",
    "    \"\"\"\n",
    "    filtered_items = [(subseq, freq) for subseq, freq in fcs_counts.items() if freq >= min_frequency]\n",
    "    filtered_items.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    data = []\n",
    "    for rank, (subseq, freq) in enumerate(filtered_items, start=1):\n",
    "        data.append({\"Subsequence\": subseq, \"Frequency\": freq, \"Rank\": rank})\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4452,
     "status": "ok",
     "timestamp": 1740731288728,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "bLH2LUtEFeCP",
    "outputId": "ad653e95-10da-48ce-c39b-6a890248cdab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting subsequences from drug SMILES...\n",
      "[DRUG SMILES] Total subsequences (frequency ≥ 5): 4229\n",
      "  Subsequence  Frequency  Rank\n",
      "0          cc        997     1\n",
      "1          O)        640     2\n",
      "2          CC        635     3\n",
      "3          C(        538     4\n",
      "4          [C        516     5\n",
      "5         [C@        516     6\n",
      "6          C@        516     7\n",
      "7         ccc        510     8\n",
      "8          H]        469     9\n",
      "9          (C        469    10\n",
      "Extracting subsequences from protein sequences...\n",
      "[PROTEIN SEQUENCES] Total subsequences (frequency ≥ 5): 46284\n",
      "  Subsequence  Frequency  Rank\n",
      "0          LL       1645     1\n",
      "1          LK       1247     2\n",
      "2          LA       1242     3\n",
      "3          AL       1192     4\n",
      "4          AA       1181     5\n",
      "5          GL       1148     6\n",
      "6          DL       1117     7\n",
      "7          LV       1109     8\n",
      "8          EL       1086     9\n",
      "9          LG       1078    10\n"
     ]
    }
   ],
   "source": [
    "drugbank_csv = DRUGBANK_CSV          # CSV file for drug SMILES\n",
    "smiles_column = \"smiles\"                  # Column name in drugbank.csv\n",
    "\n",
    "protein_csv = PROTEIN_CSV         # CSV file for protein sequences\n",
    "protein_column = \"sequence\"               # Column name in protein_sequences.csv\n",
    "\n",
    "# Parameters for subsequence extraction\n",
    "min_length = 2              # Generate subsequences of length 2 or more\n",
    "max_length = 10             # Maximum subsequence length (adjust as needed)\n",
    "min_freq_threshold = 5    # Final frequency threshold for inclusion\n",
    "\n",
    "# Pruning parameters:\n",
    "prune_every = 1000          # Prune every 1000 sequences processed\n",
    "prune_threshold = 5         # For subsequences with length >= preserve_short_length, prune if count < 5\n",
    "preserve_short_length = 5   # Always preserve subsequences with length < 5\n",
    "\n",
    "# --- PROCESS DRUG SMILES ---\n",
    "drug_output_file = DRUG_FCS_CSV #path defined\n",
    "if not os.path.exists(drug_output_file):\n",
    "    df_drug = pd.read_csv(drugbank_csv)\n",
    "    if smiles_column not in df_drug.columns:\n",
    "        raise ValueError(f\"No '{smiles_column}' column found in {drugbank_csv}!\")\n",
    "\n",
    "    smiles_list = df_drug[smiles_column].dropna().astype(str).tolist()\n",
    "    print(\"Extracting subsequences from drug SMILES...\")\n",
    "\n",
    "    smiles_fcs_counts = extract_fcs_subsequences_stream(\n",
    "        smiles_list,\n",
    "        min_length=min_length,\n",
    "        max_length=max_length,\n",
    "        prune_every=prune_every,\n",
    "        prune_threshold=prune_threshold,\n",
    "        preserve_short_length=preserve_short_length\n",
    "    )\n",
    "    smiles_fcs_df = filter_and_rank_fcs(smiles_fcs_counts, min_frequency=min_freq_threshold)\n",
    "    smiles_fcs_df.to_csv(drug_output_file, index=False)\n",
    "\n",
    "    print(f\"[DRUG SMILES] Total subsequences (frequency ≥ {min_freq_threshold}): {len(smiles_fcs_df)}\")\n",
    "    print(smiles_fcs_df.head(10))\n",
    "else:\n",
    "    print(f\"{drug_output_file} already exists. Skipping drug SMILES subsequence extraction.\")\n",
    "\n",
    "# --- PROCESS PROTEIN SEQUENCES ---\n",
    "protein_output_file = PROTEIN_FCS_CSV #path defined\n",
    "if not os.path.exists(protein_output_file):\n",
    "    df_protein = pd.read_csv(protein_csv)\n",
    "    if protein_column not in df_protein.columns:\n",
    "        raise ValueError(f\"No '{protein_column}' column found in {protein_csv}!\")\n",
    "\n",
    "    protein_list = df_protein[protein_column].dropna().astype(str).tolist()\n",
    "    print(\"Extracting subsequences from protein sequences...\")\n",
    "\n",
    "    protein_fcs_counts = extract_fcs_subsequences_stream(\n",
    "        protein_list,\n",
    "        min_length=min_length,\n",
    "        max_length=max_length,\n",
    "        prune_every=prune_every,\n",
    "        prune_threshold=prune_threshold,\n",
    "        preserve_short_length=preserve_short_length\n",
    "    )\n",
    "    protein_fcs_df = filter_and_rank_fcs(protein_fcs_counts, min_frequency=min_freq_threshold)\n",
    "    protein_fcs_df.to_csv(protein_output_file, index=False)\n",
    "\n",
    "    print(f\"[PROTEIN SEQUENCES] Total subsequences (frequency ≥ {min_freq_threshold}): {len(protein_fcs_df)}\")\n",
    "    print(protein_fcs_df.head(10))\n",
    "else:\n",
    "    print(f\"{protein_output_file} already exists. Skipping protein sequences subsequence extraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH1UhP4NzdIc"
   },
   "source": [
    "# Sequence Encoding\n",
    "\n",
    "In this section, we encode each sequence (drug SMILES or protein sequence) by mapping every valid subsequence to its corresponding token index using a token dictionary. If a subsequence is missing in the dictionary, a default value of 0 is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "yT2LoPdUv8wW"
   },
   "outputs": [],
   "source": [
    "def encode_sequence(seq, token_dict, min_subseq_len=2):\n",
    "    \"\"\"\n",
    "    Encodes a sequence (drug SMILES or protein sequence) by scanning all consecutive subsequences\n",
    "    of length >= min_subseq_len and mapping them to their token indices (or 0 if not found).\n",
    "    \"\"\"\n",
    "    return [token_dict.get(seq[i:j], 0)\n",
    "            for i in range(len(seq))\n",
    "            for j in range(i + min_subseq_len, len(seq) + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "p2untJhXv8wW"
   },
   "outputs": [],
   "source": [
    "drug_df = pd.read_csv(DRUGBANK_CSV)             # Must contain columns \"Drug id\" and \"smiles\"\n",
    "protein_df = pd.read_csv(PROTEIN_CSV)           # Must contain columns \"pbd id\" and \"sequence\"\n",
    "\n",
    "# Load token dictionaries for drugs and proteins\n",
    "drug_dict_df = pd.read_csv(DRUG_FCS_CSV)    # Columns: \"Subsequence\", \"Rank\"\n",
    "protein_dict_df = pd.read_csv(PROTEIN_FCS_CSV)       # Columns: \"Subsequence\", \"Rank\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740736528243,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "xX8G4nNQJOlR",
    "outputId": "c9804ad5-0880-4f44-f728-621b659de29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_encoded.csv and protein_encoded.csv already exist. Skipping encoding step.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Only run encoding if either output file doesn't exist.\n",
    "if not os.path.exists(DRUG_ENCODED_CSV) or not os.path.exists(PROTEIN_ENCODED_CSV):\n",
    "    # Create Python dictionaries for mapping\n",
    "    drug_token_dict = dict(zip(drug_dict_df[\"Subsequence\"], drug_dict_df[\"Rank\"]))\n",
    "    protein_token_dict = dict(zip(protein_dict_df[\"Subsequence\"], protein_dict_df[\"Rank\"]))\n",
    "\n",
    "    # Encode drug SMILES and protein sequences separately.\n",
    "    # You can adjust min_subseq_len if needed.\n",
    "    drug_df[\"Encoded\"] = drug_df[\"smiles\"].apply(lambda x: encode_sequence(x, drug_token_dict, min_subseq_len=2))\n",
    "    protein_df[\"Encoded\"] = protein_df[\"sequence\"].apply(lambda x: encode_sequence(x, protein_token_dict, min_subseq_len=2))\n",
    "\n",
    "    # print(\"Encoded drug SMILES:\")\n",
    "    # print(drug_df[[\"Drug id\", \"Encoded\"]].head())\n",
    "    # print(\"Encoded protein sequences:\")\n",
    "    # print(protein_df[[\"pdb_id\", \"Encoded\"]].head())\n",
    "\n",
    "    # Save the intermediate results to CSV for further inspection.\n",
    "    drug_df.to_csv(DRUG_ENCODED_CSV, index=False)\n",
    "    protein_df.to_csv(PROTEIN_ENCODED_CSV, index=False)\n",
    "else:\n",
    "    print(f\"{DRUG_ENCODED_CSV} and {PROTEIN_ENCODED_CSV} already exist. Skipping encoding step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "ElyrEKpOv8wY"
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Combine Encoded Sequences into BERT Input Format ---\n",
    "# Assume you have already encoded sequences stored in separate CSV files.\n",
    "# Here, we load them. We assume that the encoding is stored as a JSON-encoded list in a column.\n",
    "# If they are stored as space-separated strings, adjust the parsing accordingly.\n",
    "\n",
    "drug_encoded_df = pd.read_csv(DRUG_ENCODED_CSV)       # Columns: \"Drug id\", \"Encoded\"\n",
    "protein_encoded_df = pd.read_csv(PROTEIN_ENCODED_CSV)   # Columns: \"pbd id\", \"Encoded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1740731374429,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "63nVTqmmF5Il",
    "outputId": "817e65f4-7182-4fca-b7ae-35e92694f882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Proteins:\n",
      "   pdb_id                                            Encoded\n",
      "0   1NSI  [289, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "1   1DJL  [307, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "2   1AB2  [45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
      "3   4BSJ  [292, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "4   1FLT  [21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
      "Encoded Drugs:\n",
      "    Drug id                                            Encoded\n",
      "0  DB00131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1  DB00140  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2  DB00148  [78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
      "3  DB00159  [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4  DB00182  [44, 47, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded Proteins:\\n\",protein_encoded_df[[\"pdb_id\", \"Encoded\"]].head())\n",
    "print(\"Encoded Drugs:\\n\",drug_encoded_df[[\"Drug id\",\"Encoded\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "YK64PBzzv8wY"
   },
   "outputs": [],
   "source": [
    "# Convert the encoded column from string to list. Here we assume it's stored as JSON.\n",
    "drug_encoded_df[\"Encoded\"] = drug_encoded_df[\"Encoded\"].apply(json.loads)\n",
    "protein_encoded_df[\"Encoded\"] = protein_encoded_df[\"Encoded\"].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1740731383311,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "M-zXHMXwv8wY",
    "outputId": "e0e85ae7-0fa0-45f3-a724-706565d43065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Drug ID PDB ID\n",
      "0  DB05383   1NSI\n",
      "1  DB08814   1NSI\n",
      "2  DB08814   1MDI\n",
      "3  DB09092   1DJL\n",
      "4  DB09092   3ERY\n"
     ]
    }
   ],
   "source": [
    "pdb_drug_map = pd.read_csv(INTERACTIONS_CSV)\n",
    "if \"Unnamed: 0\" in pdb_drug_map.columns:\n",
    "    pdb_drug_map = pdb_drug_map.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Save the cleaned DataFrame back to the CSV file\n",
    "pdb_drug_map.to_csv(INTERACTIONS_CSV, index=False)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(pdb_drug_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "l77FsTjEv8wY"
   },
   "outputs": [],
   "source": [
    "# Split the \"Drug IDs\" string by \"; \" (or just \";\" if that's what's in your file)\n",
    "#pdb_drug_map[\"Drug IDs\"] = pdb_drug_map[\"Drug IDs\"].str.split(\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "nLIK3r_9v8wY"
   },
   "outputs": [],
   "source": [
    "# Explode the list so each drug gets its own row\n",
    "#pdb_drug_map = pdb_drug_map.explode(\"Drug IDs\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Pa042HArv8wY"
   },
   "outputs": [],
   "source": [
    "# Load the files\n",
    "confirmed_interactions = pd.read_csv(INTERACTIONS_CSV)\n",
    "drug_encodings = pd.read_csv(DRUG_ENCODED_CSV)\n",
    "protein_encodings = pd.read_csv(PROTEIN_ENCODED_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_neg_samples(confirmed_interactions):\n",
    "    \"\"\"\n",
    "    Generate negative samples for drug-target interactions\n",
    "    \n",
    "    Args:\n",
    "        confirmed_interactions: DataFrame with columns ['Drug id', 'pdb_id']\n",
    "    Returns:\n",
    "        DataFrame with negative samples\n",
    "    \"\"\"\n",
    "    unique_prots = confirmed_interactions[\"pdb_id\"].unique()\n",
    "    unique_drugs = confirmed_interactions[\"Drug id\"].unique()\n",
    "    \n",
    "    # Convert confirmed interactions to set for faster lookup\n",
    "    confirmed_pairs = set(zip(confirmed_interactions[\"Drug id\"], \n",
    "                            confirmed_interactions[\"pdb_id\"]))\n",
    "    \n",
    "    # Number of negative samples to generate (2x positive samples)\n",
    "    n_samples = len(confirmed_interactions) * 2\n",
    "    neg_samples = []\n",
    "    \n",
    "    with tqdm(total=n_samples, desc=\"Generating negative samples\") as pbar:\n",
    "        while len(neg_samples) < n_samples:\n",
    "            drug = random.choice(unique_drugs)\n",
    "            prot = random.choice(unique_prots)\n",
    "            \n",
    "            # Check if this pair is not in confirmed interactions\n",
    "            if (drug, prot) not in confirmed_pairs:\n",
    "                neg_samples.append([drug, prot])\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return pd.DataFrame(neg_samples, columns=[\"Drug id\", \"pdb_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "nGbuhZa4v8wZ"
   },
   "outputs": [],
   "source": [
    "# Rename columns in confirmed_interactions for consistency (if needed)\n",
    "confirmed_interactions.rename(columns={'Drug ID': 'Drug id', 'PDB ID': 'pdb_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating negative samples: 100%|██████████| 1936/1936 [00:00<00:00, 559896.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Drug id pdb_id\n",
      "0  DB01144   1I7G\n",
      "1  DB00710   1FVR\n",
      "2  DB09088   3QNZ\n",
      "3  DB03756   1F0J\n",
      "4  DB14924   1HZE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "neg_inter = gen_neg_samples(confirmed_interactions)\n",
    "print(neg_inter.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug id</th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB00786</td>\n",
       "      <td>1BQQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB01044</td>\n",
       "      <td>3FOE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB00591</td>\n",
       "      <td>1AII</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB15493</td>\n",
       "      <td>3P1N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB00485</td>\n",
       "      <td>4V11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Drug id pdb_id  label\n",
       "0  DB00786   1BQQ      1\n",
       "1  DB01044   3FOE      1\n",
       "2  DB00591   1AII      1\n",
       "3  DB15493   3P1N      0\n",
       "4  DB00485   4V11      0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_interactions['label'] = 1\n",
    "neg_inter['label'] = 0\n",
    "\n",
    "# Combine positive and negative samples\n",
    "confirmed_interactions = pd.concat([confirmed_interactions, neg_inter], ignore_index=True)\n",
    "confirmed_interactions = confirmed_interactions.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "confirmed_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "vAWtuH5Av8wZ"
   },
   "outputs": [],
   "source": [
    "# Merge the drug encodings based on Drug id\n",
    "merged_df = confirmed_interactions.merge(drug_encodings, on='Drug id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "GjK2EK1Pv8wZ"
   },
   "outputs": [],
   "source": [
    "# Merge the resulting DataFrame with protein encodings based on pdb_id\n",
    "merged_df = merged_df.merge(protein_encodings, on='pdb_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Drug id', 'pdb_id', 'label', 'smiles', 'Encoded_x', 'sequence',\n",
       "       'Encoded_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "yPucdr3qv8wZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File merged_encodings.csv already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Save the final merged file\n",
    "if not os.path.exists(MERGED_ENCODING_CSV):\n",
    "    merged_df.to_csv(MERGED_ENCODING_CSV, index=False)\n",
    "    print(f\"File {MERGED_ENCODING_CSV} created.\")\n",
    "else:\n",
    "    print(f\"File {MERGED_ENCODING_CSV} already exists. Skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "gENnErPwv8wZ"
   },
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(MERGED_ENCODING_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1740731411952,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "5DLU7Jsqv8wZ",
    "outputId": "cdeb456e-f024-4973-e309-77a3a3e9df0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Drug id pdb_id  label                                             smiles  \\\n",
      "0  DB00786   1BQQ      1  CNC(=O)[C@@H](NC(=O)[C@H](CC(C)C)[C@H](O)C(=O)...   \n",
      "1  DB01364   1BQQ      0                         CN[C@H](C)[C@H](O)c1ccccc1   \n",
      "2  DB00968   1BQQ      0                   C[C@@](N)(Cc1ccc(O)c(O)c1)C(=O)O   \n",
      "3  DB06637   1BQQ      0                                          Nc1ccncc1   \n",
      "4  DB00786   1L9K      0  CNC(=O)[C@@H](NC(=O)[C@H](CC(C)C)[C@H](O)C(=O)...   \n",
      "\n",
      "                                           Encoded_x  \\\n",
      "0  [78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "1  [78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "2  [44, 47, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 17, 49, 55, 0, 0, 0, ...   \n",
      "4  [78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "\n",
      "                                            sequence  \\\n",
      "0  IQGLKWQHNEITFCIQNYTPKVGEYATYEAIRKAFRVWESATPLRF...   \n",
      "1  IQGLKWQHNEITFCIQNYTPKVGEYATYEAIRKAFRVWESATPLRF...   \n",
      "2  IQGLKWQHNEITFCIQNYTPKVGEYATYEAIRKAFRVWESATPLRF...   \n",
      "3  IQGLKWQHNEITFCIQNYTPKVGEYATYEAIRKAFRVWESATPLRF...   \n",
      "4  ETLGEKWKSRLNALGKSEFQIYKKSGIQEVDRTLAKEGIKRGETDH...   \n",
      "\n",
      "                                           Encoded_y  \n",
      "0  [160, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
      "1  [160, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
      "2  [160, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
      "3  [160, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
      "4  [159, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "4C8jDn54v8wa"
   },
   "outputs": [],
   "source": [
    "# Define special token IDs\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1740736595909,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "zr_c0zRyv8wa",
    "outputId": "292f6bc9-b4a5-4dbd-8125-d68032fb0580"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1915: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "token = \"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\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Obobg9pSv8wa"
   },
   "outputs": [],
   "source": [
    "# Initialize DataCollator for masking\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "MwRB84zev8wb"
   },
   "outputs": [],
   "source": [
    "# def create_bert_input(drug_tokens, target_tokens):\n",
    "#     # Ensure tokens are strings and remove any existing [SEP] or [CLS]\n",
    "#     drug_tokens = [str(t) for t in drug_tokens if str(t) not in {CLS_TOKEN, SEP_TOKEN}]\n",
    "#     target_tokens = [str(t) for t in target_tokens if str(t) not in {CLS_TOKEN, SEP_TOKEN}]\n",
    "\n",
    "#     # Debug prints to inspect the tokens:\n",
    "#     #print(\"Drug tokens:\", drug_tokens)\n",
    "#     #print(\"Target tokens:\", target_tokens)\n",
    "    \n",
    "#     # Build the sequence: [CLS] drug tokens [SEP] target tokens [SEP]\n",
    "#     return [CLS_TOKEN] + drug_tokens + [SEP_TOKEN] + target_tokens + [SEP_TOKEN]\n",
    "\n",
    "def create_bert_input(drug_encoding, protein_encoding):\n",
    "    \"\"\"\n",
    "    Creates a clean BERT input sequence from drug and protein encodings\n",
    "    Args:\n",
    "        drug_encoding: encoded drug sequence\n",
    "        protein_encoding: encoded protein sequence\n",
    "    Returns:\n",
    "        list: cleaned sequence with proper BERT format\n",
    "    \"\"\"\n",
    "    # Convert JSON strings to lists if needed\n",
    "    if isinstance(drug_encoding, str):\n",
    "        drug_encoding = json.loads(drug_encoding)\n",
    "    if isinstance(protein_encoding, str):\n",
    "        protein_encoding = json.loads(protein_encoding)\n",
    "    \n",
    "    # Clean and format sequences\n",
    "    drug_seq = [str(x) for x in drug_encoding if x != 0]  # Remove padding zeros\n",
    "    protein_seq = [str(x) for x in protein_encoding if x != 0]  # Remove padding zeros\n",
    "    \n",
    "    # Create BERT sequence with proper tokens\n",
    "    sequence = (\n",
    "        ['[CLS]'] +  # Start token\n",
    "        drug_seq +   # Drug sequence\n",
    "        ['[SEP]'] +  # Separator token\n",
    "        protein_seq +  # Protein sequence\n",
    "        ['[SEP]']   # End token\n",
    "    )\n",
    "    \n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '78',\n",
       " '83',\n",
       " '4',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '15',\n",
       " '18',\n",
       " '19',\n",
       " '11',\n",
       " '12',\n",
       " '2',\n",
       " '54',\n",
       " '66',\n",
       " '67',\n",
       " '5',\n",
       " '6',\n",
       " '27',\n",
       " '31',\n",
       " '32',\n",
       " '7',\n",
       " '28',\n",
       " '33',\n",
       " '34',\n",
       " '29',\n",
       " '35',\n",
       " '36',\n",
       " '13',\n",
       " '14',\n",
       " '51',\n",
       " '9',\n",
       " '52',\n",
       " '43',\n",
       " '70',\n",
       " '83',\n",
       " '4',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '15',\n",
       " '18',\n",
       " '19',\n",
       " '11',\n",
       " '12',\n",
       " '2',\n",
       " '54',\n",
       " '66',\n",
       " '67',\n",
       " '5',\n",
       " '6',\n",
       " '38',\n",
       " '39',\n",
       " '7',\n",
       " '40',\n",
       " '41',\n",
       " '13',\n",
       " '14',\n",
       " '51',\n",
       " '9',\n",
       " '52',\n",
       " '43',\n",
       " '10',\n",
       " '3',\n",
       " '4',\n",
       " '58',\n",
       " '85',\n",
       " '10',\n",
       " '30',\n",
       " '21',\n",
       " '25',\n",
       " '21',\n",
       " '54',\n",
       " '66',\n",
       " '67',\n",
       " '5',\n",
       " '6',\n",
       " '38',\n",
       " '39',\n",
       " '7',\n",
       " '40',\n",
       " '41',\n",
       " '13',\n",
       " '14',\n",
       " '51',\n",
       " '9',\n",
       " '52',\n",
       " '43',\n",
       " '46',\n",
       " '68',\n",
       " '2',\n",
       " '77',\n",
       " '25',\n",
       " '4',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '15',\n",
       " '18',\n",
       " '19',\n",
       " '11',\n",
       " '12',\n",
       " '2',\n",
       " '63',\n",
       " '2',\n",
       " '77',\n",
       " '25',\n",
       " '4',\n",
       " '58',\n",
       " '85',\n",
       " '10',\n",
       " '30',\n",
       " '21',\n",
       " '84',\n",
       " '10',\n",
       " '30',\n",
       " '21',\n",
       " '25',\n",
       " '[SEP]',\n",
       " '160',\n",
       " '205',\n",
       " '6',\n",
       " '2',\n",
       " '342',\n",
       " '331',\n",
       " '294',\n",
       " '209',\n",
       " '79',\n",
       " '113',\n",
       " '197',\n",
       " '348',\n",
       " '328',\n",
       " '160',\n",
       " '277',\n",
       " '201',\n",
       " '195',\n",
       " '114',\n",
       " '225',\n",
       " '49',\n",
       " '52',\n",
       " '76',\n",
       " '206',\n",
       " '183',\n",
       " '102',\n",
       " '208',\n",
       " '234',\n",
       " '25',\n",
       " '64',\n",
       " '127',\n",
       " '154',\n",
       " '51',\n",
       " '130',\n",
       " '179',\n",
       " '77',\n",
       " '306',\n",
       " '324',\n",
       " '134',\n",
       " '65',\n",
       " '102',\n",
       " '114',\n",
       " '36',\n",
       " '17',\n",
       " '175',\n",
       " '179',\n",
       " '107',\n",
       " '21',\n",
       " '138',\n",
       " '283',\n",
       " '183',\n",
       " '214',\n",
       " '199',\n",
       " '127',\n",
       " '107',\n",
       " '103',\n",
       " '274',\n",
       " '295',\n",
       " '37',\n",
       " '202',\n",
       " '128',\n",
       " '58',\n",
       " '61',\n",
       " '280',\n",
       " '303',\n",
       " '178',\n",
       " '271',\n",
       " '152',\n",
       " '30',\n",
       " '103',\n",
       " '147',\n",
       " '337',\n",
       " '268',\n",
       " '47',\n",
       " '132',\n",
       " '140',\n",
       " '114',\n",
       " '229',\n",
       " '136',\n",
       " '59',\n",
       " '76',\n",
       " '103',\n",
       " '31',\n",
       " '147',\n",
       " '68',\n",
       " '3',\n",
       " '244',\n",
       " '256',\n",
       " '214',\n",
       " '285',\n",
       " '189',\n",
       " '78',\n",
       " '162',\n",
       " '243',\n",
       " '181',\n",
       " '110',\n",
       " '31',\n",
       " '47',\n",
       " '122',\n",
       " '287',\n",
       " '308',\n",
       " '136',\n",
       " '132',\n",
       " '65',\n",
       " '30',\n",
       " '213',\n",
       " '418',\n",
       " '352',\n",
       " '62',\n",
       " '100',\n",
       " '215',\n",
       " '209',\n",
       " '89',\n",
       " '7',\n",
       " '60',\n",
       " '119',\n",
       " '146',\n",
       " '265',\n",
       " '61',\n",
       " '178',\n",
       " '68',\n",
       " '8',\n",
       " '19',\n",
       " '20',\n",
       " '246',\n",
       " '295',\n",
       " '9',\n",
       " '10',\n",
       " '274',\n",
       " '256',\n",
       " '4',\n",
       " '10',\n",
       " '6',\n",
       " '12',\n",
       " '253',\n",
       " '269',\n",
       " '50',\n",
       " '97',\n",
       " '118',\n",
       " '125',\n",
       " '65',\n",
       " '64',\n",
       " '280',\n",
       " '196',\n",
       " '145',\n",
       " '229',\n",
       " '254',\n",
       " '275',\n",
       " '408',\n",
       " '278',\n",
       " '122',\n",
       " '83',\n",
       " '156',\n",
       " '249',\n",
       " '200',\n",
       " '11',\n",
       " '39',\n",
       " '95',\n",
       " '148',\n",
       " '148',\n",
       " '174',\n",
       " '150',\n",
       " '84',\n",
       " '56',\n",
       " '160',\n",
       " '210',\n",
       " '41',\n",
       " '117',\n",
       " '169',\n",
       " '31',\n",
       " '76',\n",
       " '134',\n",
       " '313',\n",
       " '325',\n",
       " '313',\n",
       " '325',\n",
       " '143',\n",
       " '85',\n",
       " '246',\n",
       " '235',\n",
       " '257',\n",
       " '210',\n",
       " '128',\n",
       " '130',\n",
       " '348',\n",
       " '382',\n",
       " '58',\n",
       " '61',\n",
       " '82',\n",
       " '44',\n",
       " '127',\n",
       " '111',\n",
       " '53',\n",
       " '51',\n",
       " '20',\n",
       " '141',\n",
       " '220',\n",
       " '81',\n",
       " '40',\n",
       " '21',\n",
       " '67',\n",
       " '132',\n",
       " '38',\n",
       " '146',\n",
       " '265',\n",
       " '61',\n",
       " '241',\n",
       " '169',\n",
       " '146',\n",
       " '176',\n",
       " '191',\n",
       " '98',\n",
       " '164',\n",
       " '108',\n",
       " '160',\n",
       " '291',\n",
       " '234',\n",
       " '79',\n",
       " '98',\n",
       " '202',\n",
       " '207',\n",
       " '98',\n",
       " '297',\n",
       " '383',\n",
       " '204',\n",
       " '75',\n",
       " '162',\n",
       " '95',\n",
       " '227',\n",
       " '221',\n",
       " '61',\n",
       " '116',\n",
       " '166',\n",
       " '228',\n",
       " '241',\n",
       " '195',\n",
       " '92',\n",
       " '145',\n",
       " '99',\n",
       " '5',\n",
       " '366',\n",
       " '5',\n",
       " '404',\n",
       " '20',\n",
       " '317',\n",
       " '310',\n",
       " '33',\n",
       " '54',\n",
       " '14',\n",
       " '26',\n",
       " '61',\n",
       " '110',\n",
       " '31',\n",
       " '57',\n",
       " '81',\n",
       " '40',\n",
       " '206',\n",
       " '120',\n",
       " '34',\n",
       " '74',\n",
       " '23',\n",
       " '57',\n",
       " '51',\n",
       " '30',\n",
       " '103',\n",
       " '146',\n",
       " '119',\n",
       " '146',\n",
       " '321',\n",
       " '296',\n",
       " '113',\n",
       " '15',\n",
       " '273',\n",
       " '311',\n",
       " '137',\n",
       " '228',\n",
       " '82',\n",
       " '138',\n",
       " '418',\n",
       " '389',\n",
       " '122',\n",
       " '15',\n",
       " '13',\n",
       " '65',\n",
       " '102',\n",
       " '250',\n",
       " '198',\n",
       " '81',\n",
       " '106',\n",
       " '14',\n",
       " '60',\n",
       " '315',\n",
       " '242',\n",
       " '217',\n",
       " '275',\n",
       " '341',\n",
       " '282',\n",
       " '290',\n",
       " '335',\n",
       " '338',\n",
       " '345',\n",
       " '71',\n",
       " '113',\n",
       " '219',\n",
       " '381',\n",
       " '298',\n",
       " '307',\n",
       " '303',\n",
       " '105',\n",
       " '199',\n",
       " '80',\n",
       " '50',\n",
       " '143',\n",
       " '95',\n",
       " '112',\n",
       " '338',\n",
       " '260',\n",
       " '318',\n",
       " '278',\n",
       " '364',\n",
       " '362',\n",
       " '63',\n",
       " '83',\n",
       " '37',\n",
       " '177',\n",
       " '181',\n",
       " '173',\n",
       " '119',\n",
       " '274',\n",
       " '357',\n",
       " '128',\n",
       " '53',\n",
       " '223',\n",
       " '271',\n",
       " '152',\n",
       " '286',\n",
       " '328',\n",
       " '98',\n",
       " '164',\n",
       " '123',\n",
       " '97',\n",
       " '59',\n",
       " '45',\n",
       " '313',\n",
       " '334',\n",
       " '370',\n",
       " '247',\n",
       " '84',\n",
       " '48',\n",
       " '5',\n",
       " '145',\n",
       " '157',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = create_bert_input(merged_df[\"Encoded_x\"].iloc[0], merged_df[\"Encoded_y\"].iloc[0])\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "9-frHZlmv8wb"
   },
   "outputs": [],
   "source": [
    "def process_chunk(chunk, tokenizer=None, data_collator=None):\n",
    "    \"\"\"\n",
    "    Process a chunk of drug-target pairs using create_bert_input function\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    if data_collator is None:\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, \n",
    "            mlm=True, \n",
    "            mlm_probability=0.15\n",
    "        )\n",
    "    \n",
    "    processed_sequences = []\n",
    "    \n",
    "    # Process each row in the chunk\n",
    "    for idx in range(len(chunk)):\n",
    "        # Get drug and protein encodings\n",
    "        drug_encoding = chunk['Encoded_x'].iloc[idx]\n",
    "        protein_encoding = chunk['Encoded_y'].iloc[idx]\n",
    "        \n",
    "        # Create BERT input sequence\n",
    "        sequence = create_bert_input(drug_encoding, protein_encoding)\n",
    "        \n",
    "        # Convert sequence to string format\n",
    "        sequence_str = ' '.join(sequence)\n",
    "        \n",
    "        # Apply masking using data_collator\n",
    "        features = [{\"input_ids\": tokenizer.convert_tokens_to_ids(sequence)}]\n",
    "        masked = data_collator(features)\n",
    "        \n",
    "        # Convert masked ids back to tokens and join\n",
    "        masked_sequence = ' '.join(map(str, masked['input_ids'][0].tolist()))\n",
    "        processed_sequences.append(masked_sequence)\n",
    "    \n",
    "    return pd.DataFrame({\"Masked_Input\": processed_sequences})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1740739522504,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "61s0IQzqv8wb",
    "outputId": "b8327ee1-3ec2-45bd-f70f-9b02517359ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping processing.\n",
      "\n",
      "Verification:\n",
      "Number of stored sequences: 2894\n",
      "Columns in storage: ['Masked_Input']\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('final_bert_inputs_masked.h5'):\n",
    "    chunksize = 10  # Reduced chunk size\n",
    "    \n",
    "    # Calculate maximum string length from first few chunks\n",
    "    print(\"Calculating maximum sequence length...\")\n",
    "    max_len = 0\n",
    "    sample_chunks = pd.read_csv(\"merged_encodings.csv\", chunksize=chunksize, nrows=50)\n",
    "    \n",
    "    for chunk in sample_chunks:\n",
    "        processed = process_chunk(chunk, tokenizer, data_collator)\n",
    "        chunk_max = processed['Masked_Input'].str.len().max()\n",
    "        max_len = max(max_len, chunk_max)\n",
    "    \n",
    "    # Add safety margin to max_len\n",
    "    max_len = 20000\n",
    "    print(f\"Maximum sequence length (with buffer): {max_len}\")\n",
    "    \n",
    "    # Set min_itemsize with the calculated max_len\n",
    "    min_itemsize = {\n",
    "        'Masked_Input': max_len,\n",
    "        'index': 100  # for index column if needed\n",
    "    }\n",
    "    \n",
    "    with pd.HDFStore('final_bert_inputs_masked.h5', mode='w') as store:\n",
    "        # Get total chunks for progress bar\n",
    "        total_chunks = sum(1 for _ in pd.read_csv(\"merged_encodings.csv\", chunksize=chunksize))\n",
    "        \n",
    "        # Process chunks with progress bar\n",
    "        for i, chunk in enumerate(tqdm(pd.read_csv(\"merged_encodings.csv\", chunksize=chunksize), \n",
    "                                     total=total_chunks, \n",
    "                                     desc=\"Processing chunks\")):\n",
    "            try:\n",
    "                processed_chunk = process_chunk(chunk, tokenizer, data_collator)\n",
    "                \n",
    "                # Store with explicit min_itemsize\n",
    "                store.append('df', \n",
    "                           processed_chunk, \n",
    "                           format='table', \n",
    "                           data_columns=True,\n",
    "                           min_itemsize=min_itemsize,\n",
    "                           index=False)  # Disable index storage\n",
    "                \n",
    "                # Memory cleanup\n",
    "                del processed_chunk\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # Progress update every 10 chunks\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"\\nProcessed {i+1}/{total_chunks} chunks\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in chunk {i}: {str(e)}\")\n",
    "                print(\"Continuing with next chunk...\")\n",
    "                continue\n",
    "            \n",
    "    print(\"\\nProcessing complete! Data saved to final_bert_inputs_masked.h5\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping processing.\")\n",
    "\n",
    "# Verify the saved data\n",
    "def verify_data():\n",
    "    with pd.HDFStore('final_bert_inputs_masked.h5', 'r') as store:\n",
    "        print(f\"\\nVerification:\")\n",
    "        print(f\"Number of stored sequences: {len(store['df'])}\")\n",
    "        print(f\"Columns in storage: {store['df'].columns.tolist()}\")\n",
    "        \n",
    "verify_data()\n",
    "###\n",
    "### Took 80 minutes to run the above code\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sucj1bx83mPe"
   },
   "source": [
    "# Model Training\n",
    "Training the BERT model by iterating over the DataLoader for a set number of epochs.\n",
    "Before training we are optimizing the input texts and labels to pass through Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3893,
     "status": "ok",
     "timestamp": 1740731417410,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "YIFxvvYov8wc",
    "outputId": "4432dec1-3a97-4d35-8fc1-f2fcce9145fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.11/site-packages (3.12.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/anaconda3/lib/python3.11/site-packages (from h5py) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "tYY-2Q7yv8wc"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1740735156917,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "X83SSDOjv8wc",
    "outputId": "fa303337-cfcc-44f9-e9e2-aa614bba843a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file structure:\n",
      "df: <class 'h5py._hl.group.Group'>\n",
      "  table: <class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "def print_hdf5_structure(obj, indent=0):\n",
    "    for key in obj.keys():\n",
    "        item = obj[key]\n",
    "        print(\"  \" * indent + f\"{key}: {type(item)}\")\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print_hdf5_structure(item, indent+1)\n",
    "\n",
    "with h5py.File('final_bert_inputs_masked.h5', 'r') as f:\n",
    "    print(\"HDF5 file structure:\")\n",
    "    print_hdf5_structure(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1740734424556,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "nlyigAOuv8wd",
    "outputId": "fa0d1e10-047c-42f6-9a26-5759b737f8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries:\n",
      "[(0, b'101 6275 103 103 2570 2603 2484 2321 2324 2539 2340 2260 1016 5139 5764 6163 1019 1020 2676 103 3590 1021 2654 3943 4090 2756 3486 103 2410 2403 4868 1023 103 4724 3963 6640 1018 2570 2603 2484 2321 2324 2539 2340 2260 1016 5139 5764 6163 1019 1020 4229 4464 103 103 4601 2410 2403 4868 1023 4720 4724 2184 1017 1018 5388 103 2184 2382 2538 2423 2538 5139 5764 6163 1019 1020 4229 4464 1021 2871 4601 2410 2403 4868 1023 4720 4724 4805 6273 1016 6255 2423 1018 2570 2603 2484 2321 103 2539 2340 103 1016 103 1016 6255 2423 1018 5388 5594 2184 2382 2538 15114 2184 103 2538 2423 102 8148 16327 1020 1016 100 103 28135 19348 6535 103 103 100 25256 8148 25578 16345 17317 12457 14993 4749 4720 6146 18744 103 9402 18512 22018 2423 18545 13029 16666 4868 7558 103 6255 24622 27234 15170 103 9402 12457 4029 2459 12862 20311 103 2538 15028 25504 103 19936 20713 13029 10550 9800 25586 21679 103 16798 11899 5388 6079 13427 19988 103 25103 15017 2382 9800 103 28489 25143 4700 14078 8574 12457 24331 15407 5354 6146 9800 103 103 6273 1017 24194 17273 19936 21777 20500 6275 17832 22884 18596 7287 2861 4700 103 23090 24232 15407 14078 3515 2382 19883 100 28906 5786 2531 17405 19348 6486 1021 3438 13285 16333 20549 6079 19289 6273 1022 103 2322 22376 21679 1023 2184 103 17273 1018 2184 1020 2260 103 25717 2753 5989 12963 8732 3515 4185 13427 20035 103 103 22234 103 100 24709 13092 103 16734 103 3263 2340 10373 5345 3116 16459 19492 5018 6391 5179 8148 12875 4601 12567 18582 103 6146 15170 22997 103 22997 19652 16065 103 22376 17825 24368 12875 11899 7558 100 100 5388 6079 6445 4008 13029 11118 5187 4868 2322 15471 10545 6282 2871 103 6163 14078 4229 16333 20549 6079 103 18582 103 18561 19871 5818 17943 10715 8148 27173 103 6535 5818 16798 103 5818 27502 100 19627 4293 17832 5345 21489 103 6079 12904 18610 22238 22343 17317 6227 13741 103 1019 100 103 24837 2322 26628 103 103 5139 2403 2656 6079 7287 2861 5401 6282 2871 18744 6036 103 103 2603 22312 103 2382 9800 16333 13285 16333 24030 27200 12104 2321 25371 23532 14989 22238 6445 15028 100 100 13092 2321 2410 3515 9402 5539 20003 6282 10114 2403 3438 22904 22431 103 17528 28358 103 103 24426 27908 23785 6390 12104 20636 29335 6337 24559 19988 8746 20713 3770 2753 16065 103 11176 27908 103 27003 24709 100 100 6191 6640 4261 18118 18596 19410 13285 25586 103 103 5187 20802 25103 15017 24921 25256 5818 17943 13138 5989 5354 3429 22997 29562 16444 23380 6391 4466 1019 13741 17403 102')\n",
      " (1, b'101 6275 1019 1020 4229 4464 1021 2871 4601 2410 2403 4868 103 4720 4724 2184 2382 2538 5139 103 6163 1019 103 4229 4464 1021 2871 4601 2410 2403 4868 1023 4720 4724 4805 6273 1016 5786 103 103 2459 4749 4583 5824 4413 5187 6421 1015 1022 103 103 1022 4261 103 1022 1015 6564 103 102 8148 16327 103 1016 100 27533 28135 19348 6535 12104 19975 100 25256 8148 25578 103 17317 12457 14993 4749 4720 6146 18744 18677 9402 18512 22018 2423 4185 13029 16666 103 7558 20311 103 24622 103 15170 3515 9402 12457 4029 2459 103 20311 103 2538 15028 103 18677 103 20713 13029 10550 9800 25586 21679 4261 103 11899 5388 6079 13427 19988 19289 25103 15017 2382 9800 103 28489 25143 4700 14078 8574 12457 103 15407 5354 6146 103 2861 16471 6273 1017 24194 103 19936 103 103 103 17832 22884 103 7287 103 4700 13092 23090 24232 15407 14078 3515 2382 19883 100 13738 5786 103 17405 19348 6486 1021 3438 13285 16333 20549 6079 19289 6273 1022 2539 2322 22376 21679 1023 2184 25586 17273 1018 2184 1020 2260 23254 103 2753 5989 103 8732 3515 4185 13427 20035 103 22777 22234 14495 100 24709 13092 6640 16734 25847 3263 2340 4464 5345 16459 16459 19492 5018 6391 5179 8148 12875 4601 12567 18582 2861 6146 15170 103 19652 22997 19652 16065 5594 103 103 24368 12875 11899 7558 100 100 5388 6079 6445 4008 13029 103 5187 4868 2322 15471 10545 6282 2871 2538 6163 103 4229 16333 20549 103 22343 18582 16333 18561 19871 5818 17943 10715 8148 27173 22018 6535 5818 16798 19843 5818 27502 100 19627 103 17832 5345 1473 19594 6079 12904 18610 22238 22343 17317 6227 13741 5585 1019 100 1019 24837 2322 26628 17196 3943 5139 103 2656 6079 7287 2861 5401 6282 2871 18744 6036 4090 6356 103 5401 4868 103 103 16333 13285 16333 24030 103 12104 2321 25371 23532 14989 103 6445 15028 100 100 13092 2321 2410 103 9402 5539 20003 6282 10114 2403 3438 22904 22431 103 17528 28358 103 17222 24426 27908 103 6390 9963 4191 29335 27240 24559 103 8746 103 3770 2753 16065 5345 11176 27908 13539 27003 24709 100 100 6191 6640 4261 103 18596 19410 103 8739 26231 11899 5187 20802 25103 15017 24921 25256 5818 17943 103 5989 5354 3429 103 29562 16444 23380 6391 4466 1019 13741 17403 102')\n",
      " (2, b'101 4008 4700 4466 1019 1020 2676 1021 103 2756 4724 3963 6391 2184 2459 4749 4583 5824 4413 5187 6421 1015 1022 6353 1015 3429 2322 103 6273 1016 5786 103 2322 4805 6273 1016 5786 2385 3770 2459 2423 1018 2570 2603 2484 2321 2324 2539 6070 103 2260 103 1016 6445 6390 102 8148 16327 1020 1016 100 27533 28135 103 6535 12104 19975 100 25256 8148 25578 16345 17317 103 14993 4749 4720 6146 18744 18677 9402 103 22018 2423 4185 13029 16666 4868 7558 20311 6255 24622 27234 103 3515 9402 12457 4029 2459 12862 20311 103 103 103 103 18677 19936 20713 103 23439 9800 25586 21679 103 16798 11899 5388 6079 103 19988 19289 25103 15017 2382 9800 16471 28489 25143 4700 103 8574 12457 22777 15407 5354 6146 9800 2861 16471 6273 1017 24194 17273 19936 21777 20500 6275 17832 22884 18596 7287 2861 4700 103 23090 103 103 103 103 2382 19883 100 103 5786 2531 17405 19348 6486 1021 3438 13285 16333 20549 6079 19289 6273 1022 2539 2322 103 103 1023 2184 25586 17273 1018 103 103 2260 23254 25717 2753 5989 12963 8732 3515 4185 13427 20035 13741 22777 22234 103 100 24709 13092 6640 103 23628 3263 2340 4464 5345 16459 16459 103 5018 6391 5179 8148 12875 4601 12567 18582 103 6146 15170 22997 19652 22997 19652 103 5594 22376 17825 103 12875 11899 7558 100 100 5388 6079 6445 103 13029 11118 5187 4868 2322 103 10545 6282 2871 2538 6163 14078 103 16333 20549 6079 22343 18582 16333 18561 19871 5818 17943 10715 8148 27173 22018 6535 5818 103 19843 5818 27502 100 19627 4293 17832 5345 21489 19594 6079 12904 18610 22238 22343 17317 103 13741 5585 1019 100 1019 24837 2322 103 17196 3943 5139 2403 2656 6079 103 2861 5401 6282 2871 103 6036 4090 6356 2603 5401 4868 2382 9800 16333 13285 103 19685 27200 12104 8090 25371 23532 14989 22238 6445 15028 100 100 13092 2321 2410 3515 9402 5539 20003 6282 10114 2403 3438 22904 22431 20335 17528 28358 26267 17222 103 27908 23785 6390 12104 20636 29335 27240 24559 103 8746 20713 3770 2753 16065 5345 103 27908 13539 27003 24709 100 100 6191 6640 4261 18118 18596 19410 13285 25586 26231 11899 5187 20802 25103 15017 24921 25256 5818 17943 13138 5989 5354 3429 22997 29562 103 23380 103 4466 1019 13741 17403 102')\n",
      " (3, b'101 2459 4749 103 4413 5187 1015 4293 1015 6564 2459 102 8148 16327 1020 1016 100 27533 103 19348 6535 12104 19975 100 25256 8148 25578 103 103 12457 14993 4749 4720 6146 18744 18677 9402 18512 22018 2423 4185 13029 16666 4868 7558 20311 6255 24622 27234 652 3515 9402 12457 4029 2459 103 20311 10550 2538 15028 25504 103 19936 20713 13029 10550 103 25586 21679 103 16798 11899 5388 6079 13427 19988 19289 25103 15017 2382 9800 16471 28489 25143 103 14078 8574 12457 22777 15407 5354 6146 9800 2861 16471 6273 1017 103 103 19936 21777 20500 6275 103 22884 18596 7287 2861 4700 103 23090 24232 15407 14078 3515 2382 103 100 28906 5786 2531 17405 103 103 1021 3438 13285 16333 20549 103 103 6273 1022 2539 2322 22376 21679 103 2184 103 17273 103 2184 1020 2260 23254 25717 2753 5989 12963 8732 3515 4185 13427 20035 13741 22777 22234 17528 100 24709 13092 6640 16734 23628 103 103 4464 5345 16459 16459 19492 5018 6391 5179 8148 12875 4601 12567 18582 2861 6146 103 22997 19652 22997 19652 16065 5594 22376 17825 24368 12875 11899 7558 100 100 5388 6079 6445 4008 103 11118 5187 103 2322 15471 10545 6282 2871 2538 6163 14078 4229 103 20549 103 22343 18582 16333 18561 103 5818 17943 10715 8148 27173 22018 6535 103 16798 103 5818 27502 100 19627 4293 17832 5345 21489 19594 6079 12904 103 22238 22343 17317 6227 13741 5585 1019 100 1019 24837 2322 26628 17196 103 5139 2403 2656 17167 7287 2861 5401 6282 2871 18744 6036 4090 6356 103 5401 4868 2382 103 16333 13285 16333 24030 27200 12104 2321 25371 23532 14989 22238 6445 15028 100 100 13092 2321 2410 3515 9402 5539 20003 6282 103 2403 3438 22904 22431 20335 17528 9815 26267 103 24426 27908 23785 6390 12104 20636 29335 27240 24559 19988 8746 20713 103 103 16065 5345 11176 27908 13539 27003 24709 100 100 6191 6640 4261 18118 103 19410 13285 25586 26231 103 5187 20802 25103 15017 24921 25256 5818 17943 13138 5989 5354 3429 22997 29562 16444 23380 6391 103 103 13741 103 102')\n",
      " (4, b'101 6275 9687 1018 2570 103 2484 103 2324 2539 2340 2260 1016 5139 5764 6163 1019 1020 2676 2861 3590 103 2654 103 4090 2756 3486 4029 2410 2403 4868 1023 4720 4724 3963 6640 1018 2570 2603 2484 2321 2324 2539 2340 2260 1016 5139 5764 103 1019 1020 4229 4464 1021 2871 4601 2410 2403 4868 103 4720 4724 2184 1017 1018 5388 5594 2184 2382 2538 2423 103 5139 5764 6163 1019 1020 103 4464 1021 2871 4601 2410 2403 4868 103 103 4724 4805 6273 1016 6255 2423 1018 2570 2603 2484 2321 2324 2539 2340 2260 103 6191 103 6255 2423 1018 5388 5594 103 2382 2538 6391 2184 2382 2538 2423 102 18914 2321 2184 6146 4261 100 3998 10114 16621 2484 3438 1018 2184 5401 10114 6365 18610 21950 19843 22343 20666 6282 103 4229 5179 8148 103 2538 6163 4427 15376 2321 103 5187 2871 9800 5179 103 17943 103 6146 18914 14748 25797 29327 17273 2322 103 103 6391 3429 3515 5187 2385 2459 100 100 3263 2676 5824 17405 3590 1022 103 12457 6353 9800 5401 4749 2756 6163 1021 2184 17222 17196 9645 6391 2861 28469 28188 22115 25745 19342 17196 2861 1020 103 18118 14378 2531 10550 2538 4724 4293 1020 103 16785 4293 103 17832 6275 103 21679 103 19883 19871 8746 24559 19123 8574 18512 18582 28469 26271 103 1022 2531 2484 2654 11816 103 3943 6163 6070 14506 103 22238 8746 17403 6353 5824 29335 23532 13092 2321 1015 25371 23532 103 7287 6146 15170 2753 16065 22884 18561 17691 5786 2676 100 2423 2603 103 15376 2321 2459 103 2340 3438 3590 1022 27433 2676 16734 100 29567 103 17867 20637 21035 5539 1113 100 26499 103 4749 2340 3438 18561 25504 100 19123 2753 6421 4008 12904 4261 27502 23688 103 100 1018 2654 18225 16666 23297 25143 103 4466 103 103 2531 17405 18561 4029 2410 16621 17405 10894 8574 23090 21679 25191 103 19151 100 5139 17867 4413 4229 16333 18596 6445 5139 2753 6421 103 24030 19988 3770 16621 25392 19955 4090 19410 22898 103 18164 23980 21211 103 20405 7693 4868 103 18512 22018 19883 5345 6070 6163 1021 2184 3429 4229 6205 20636 17405 18596 7287 5179 102')\n",
      " (5, b'101 103 5401 103 5401 103 5401 5401 1017 103 1017 1018 2570 2603 103 2321 103 2539 6070 103 2260 6584 1016 6445 6390 102 18914 2321 2184 6146 4261 100 3998 10114 16621 23340 3438 1018 2184 5401 10114 6365 18610 21950 103 22343 20666 103 10114 4229 5179 8148 103 2538 6163 19492 15376 2321 1017 5187 2871 9800 5179 5818 17943 6391 103 18914 14748 25797 29327 17273 2322 5139 16621 6391 3429 3515 5187 2385 2459 100 100 3263 2676 5824 17405 3590 1022 6191 12457 6353 9800 5401 4749 2756 6163 103 2184 17222 103 2217 6391 2861 28469 28188 22115 103 19342 17196 2861 1020 1016 103 14378 2531 10550 2538 4724 4293 1020 2324 16785 103 2861 17832 6275 25586 21679 3486 19883 19871 8746 24559 19123 8574 18512 18582 28469 26271 3590 1022 2531 103 2654 11816 103 3943 103 6070 14506 25103 22238 8746 17403 6353 5824 29335 23532 13092 103 1015 25371 103 6079 7287 6146 103 2753 16065 22884 18561 17691 5786 2676 100 2423 2603 103 15376 2321 2459 103 2340 103 103 1022 103 2676 16734 100 29567 2410 17867 20637 21035 5539 26871 100 26499 4724 4749 2340 3438 18561 25504 100 19123 2753 6421 4008 151 4261 27502 23688 2423 100 103 2654 18225 16666 23297 25143 2861 103 1018 1022 2531 17405 18561 4029 2410 16621 103 10894 8574 23090 21679 25191 27878 19151 100 103 17867 4413 4229 16333 18596 6445 103 2753 6421 15471 24030 19988 3770 16621 25392 19955 4090 19410 22898 103 18164 23980 21211 103 103 6282 4868 9402 18512 103 19883 5345 6070 6163 1021 103 3429 4229 103 20636 17405 18596 7287 5179 102')\n",
      " (6, b'101 1017 103 5401 5401 103 103 1017 2753 103 2753 1017 1018 2570 2603 2484 2321 2324 2539 6070 2340 2260 6584 1016 6445 6390 102 18914 2321 2184 6146 17130 100 3998 10114 103 2484 3438 1018 2184 5401 10114 6365 18610 21950 19843 22343 20666 6282 10114 4229 5179 8148 103 2538 103 19492 15376 103 1017 5187 2871 9800 5179 5818 17943 6391 6146 18914 14748 25797 29327 103 2322 5139 16621 6391 3429 3515 103 2385 2459 100 100 3263 2676 5824 17405 3590 1022 103 12457 6353 9800 5401 4749 2756 6163 1021 2184 17222 103 9645 103 2861 1775 28188 22115 25745 19342 17196 2861 1020 1016 18118 14378 103 103 2538 103 4293 1020 2324 16785 4293 2861 17832 6275 103 21679 3486 19883 19871 8746 24559 103 8574 18512 103 28469 26271 3590 1022 2531 2484 2654 11816 4229 3943 6163 6070 14506 25103 22238 8746 17403 6353 5824 29335 23532 13092 103 1015 25371 103 6079 103 6146 15170 2753 16065 22884 18561 103 103 2676 100 2423 2603 9645 15376 2321 103 103 2340 3438 3590 1022 27433 103 16734 100 103 2410 17867 20637 21035 5539 26871 100 26499 13062 103 103 3438 18561 25504 100 19123 2753 6421 4008 12904 4261 27502 23688 2423 100 1018 2654 18225 16666 23297 25143 2861 4466 1018 1022 2531 103 18561 4029 2410 16621 17405 10894 8574 23090 21679 103 27878 103 100 5139 103 103 4229 16333 18596 6445 5139 2753 6421 15471 24030 19988 3770 16621 25392 103 4090 103 22898 12862 18164 103 21211 27054 20405 103 103 9402 18512 103 19883 5345 6070 6163 1021 2184 3429 4229 6205 20636 103 18596 7287 5179 102')\n",
      " (7, b'101 6640 1018 2570 2603 2484 2321 2324 2539 103 2260 1016 5786 2385 3770 2459 4293 1019 1020 4229 4464 1021 2871 4601 103 2403 1023 1019 1020 2676 2861 3590 1021 2654 3943 4090 2756 3486 4029 2410 2403 4868 1023 4720 4724 2184 6282 1016 5139 5764 6163 1019 1020 2676 2861 3590 1021 2654 28602 4090 2756 103 103 103 2403 4868 1023 4720 4724 4805 6273 103 5139 103 6163 1019 1020 4229 4464 1021 2871 4601 2410 2403 1023 1016 102 18914 2321 2184 6146 4261 100 3998 10114 16621 103 3438 1018 2184 5401 10114 6365 18610 21950 19843 983 20666 6282 10114 4229 5179 8148 13913 2538 6163 19492 15376 2321 1017 5187 2871 9800 5179 5182 17943 6391 6146 103 14748 25797 29327 17273 2322 5139 16621 6391 3429 3515 5187 103 2459 100 100 3263 2676 5824 17405 3590 103 6086 12457 103 9800 5401 4749 2756 6163 1021 103 17222 17196 103 6391 103 28469 103 22115 25745 19342 17196 2861 1020 103 18118 14378 2531 10550 2538 4724 4293 1020 2324 16785 4293 2861 17832 6275 25586 21679 3486 19883 19871 8746 24559 19123 8574 18512 18582 28469 26271 3590 1022 2531 2484 2654 11816 4229 3943 6163 103 14506 25103 22238 8746 17403 103 5824 29335 23532 13092 2321 1015 25371 23532 103 7287 6146 103 2753 103 22884 18561 17691 5786 2676 100 2423 2603 9645 15376 2321 103 6255 2340 3438 3590 103 27433 2676 16734 100 29567 2410 17867 20637 21035 103 26871 100 26499 4724 4749 2340 3438 18561 25504 100 19123 2753 6421 4008 103 4261 27502 23688 2423 100 1018 2654 18225 16666 103 25143 2861 4466 1018 103 2531 17405 18561 4029 2410 16621 17405 10894 8574 23090 21679 25191 27878 19151 100 5139 17867 4413 4229 16333 18596 6445 5139 2753 6421 15471 24030 19988 3770 16621 103 19955 4090 19410 22898 12862 18164 23980 21211 27054 20405 6282 4868 9402 30511 22018 19883 5345 6070 6163 1021 2184 3429 4229 6205 20636 17405 18596 7287 5179 102')\n",
      " (8, b'101 6282 2459 4749 4413 2322 2184 103 2538 2385 2656 4185 5354 2322 2322 4805 6273 1016 5786 103 3770 2459 103 5401 103 2538 103 103 2753 1017 1018 2570 2603 2484 2321 2324 103 6070 2340 2260 103 103 6445 6390 1016 103 2423 1018 2570 2603 2484 2321 2324 103 6070 2340 2260 6584 1016 6445 6390 3438 102 103 2321 2184 6146 4261 100 3998 10114 16621 2484 3438 1018 2184 5401 10114 6365 18610 21950 19843 22343 20666 6282 10114 4229 5179 8148 13913 2538 6163 19492 15376 2321 1017 5187 2871 9800 5179 103 17943 6391 6146 18914 14748 25797 103 103 2322 5139 16621 6391 103 103 5187 2385 103 100 100 3263 2676 103 17405 3590 1022 6191 12457 6353 9800 5401 4749 2756 6163 1021 2184 17222 17196 9645 6391 103 28469 28188 22115 25745 19342 17196 2861 1020 1016 18118 14378 2531 10550 2538 4724 4293 1020 2324 16785 4293 2861 17832 6275 25586 21679 3486 19883 19871 8746 24559 19123 8574 18512 18582 28469 26271 3590 1022 2531 103 2654 11816 4229 3943 6163 6070 14506 103 22238 8746 17403 6353 5824 29335 27409 13092 2321 1015 25371 23532 6079 103 6146 15170 2753 16065 22884 18561 17691 5786 2676 100 2423 2603 9645 103 280 2459 6255 2340 3438 3590 1022 27433 2676 16734 100 29567 2410 17867 20637 21035 5539 26871 100 26499 4724 4749 103 103 18561 25504 100 19123 2753 6421 4008 12904 4261 27502 23688 2423 100 1018 2654 103 16666 23297 25143 2861 4466 1018 1022 2531 17405 18561 4029 2410 16621 17405 10894 8574 23090 103 25191 27878 19151 100 5139 17867 4413 4229 16333 18596 6445 5139 2753 6421 15471 24030 19988 3770 16621 103 19955 4090 19410 22898 12862 18164 23980 103 27054 20405 6282 4868 9402 18512 22018 19883 5345 6070 6163 1021 2184 3429 4229 6205 20636 17405 103 7287 5179 102')\n",
      " (9, b'101 1017 1018 2570 2603 2484 2321 2324 2539 2340 2260 1016 6191 6079 1017 6275 2656 4185 5354 1015 1022 6353 1015 3429 2322 4805 3438 4008 4700 4466 1019 1020 2676 2861 3590 103 2654 3943 103 103 3486 103 2410 103 1023 6282 1019 1020 2676 1021 2654 2756 4724 2184 1015 4293 6391 1015 1022 6353 1015 3429 2455 2184 2385 6356 1015 6390 2385 6356 103 2656 2423 1017 102 18914 2321 2184 6146 4261 100 28104 10114 16621 2484 103 1018 2184 5401 10114 6365 18610 21950 19843 103 20666 6282 103 4229 103 8148 13913 2538 6163 103 15376 2321 1017 5187 2871 9800 5179 5818 17943 6391 6146 103 103 25797 29327 17273 103 5139 16621 6391 3429 3515 103 2385 2459 100 100 103 103 5824 17405 3590 103 6191 12457 103 9800 103 4749 2756 6163 103 2184 17222 17196 9645 6391 2861 103 28188 103 25745 19342 17196 2861 1020 1016 18118 15487 2531 10550 2538 103 14432 1020 2324 16785 4293 2861 17832 6275 103 21679 3486 19883 19871 8746 13974 19123 8574 18512 18582 103 103 3590 1022 2531 2484 2654 11816 4229 3943 103 6070 14506 25103 22238 8746 17403 6353 5824 29335 23532 13092 2321 1015 25371 23532 6079 103 6146 15170 103 16065 22884 18561 17691 5786 2676 100 2423 2603 9645 15376 2321 18233 6255 2340 103 3590 1022 27433 2676 25237 100 29567 2410 17867 20637 21035 5539 26871 100 26499 4724 4749 2340 3438 18561 25504 100 19123 103 6421 4008 12904 4261 27502 23688 2423 100 1018 2654 18225 16666 103 25143 103 2739 103 1022 2531 17405 18561 4029 2410 16621 17405 10894 8574 23090 21679 25191 27878 19151 100 103 103 4413 103 16333 103 6445 5139 2753 103 15471 24030 103 3770 16621 25392 19955 21148 19410 22898 12862 18164 23980 21211 103 20405 6282 4868 9402 18512 22018 103 5345 6070 6163 1021 2184 3429 103 6205 103 17405 103 7287 5179 102')]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('final_bert_inputs_masked.h5', 'r') as f:\n",
    "    # Access the final table dataset\n",
    "    dataset = f[\"df\"][\"table\"]\n",
    "\n",
    "    # Print the first 10 entries\n",
    "    print(\"First 10 entries:\")\n",
    "    print(dataset[:10])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740734426927,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "81875UVRv8wd",
    "outputId": "878990ea-6af9-4d90-8031-4f9e32b48d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 2894\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('final_bert_inputs_masked.h5', 'r') as f:\n",
    "    dataset = f[\"df\"][\"table\"]\n",
    "    num_entries = dataset.shape[0]\n",
    "    print(\"Number of entries:\", num_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "vM1nOofjv8wf"
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Load Masked Inputs from HDF5 and Pair IDs from CSV ---\n",
    "masked_df = pd.read_hdf(\"final_bert_inputs_masked.h5\", key=\"df/table\")\n",
    "pairs_df = pd.read_csv(\"merged_encodings.csv\")  # Should include columns \"Drug id\" and \"pbd id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1740736611073,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "oR5_LZtSv8wf",
    "outputId": "d50c5cd6-fc07-4829-955c-ce7e8de0ff95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    101 6275 103 103 2570 2603 2484 2321 2324 2539...\n",
      "1    101 6275 1019 1020 4229 4464 1021 2871 4601 24...\n",
      "2    101 4008 4700 4466 1019 1020 2676 1021 103 275...\n",
      "3    101 2459 4749 103 4413 5187 1015 4293 1015 656...\n",
      "4    101 6275 9687 1018 2570 103 2484 103 2324 2539...\n",
      "Name: Masked_Input, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assign the masked inputs (assumed column name \"Masked_Input\") from masked_df to pairs_df\n",
    "pairs_df[\"Masked_Input\"] = masked_df[\"Masked_Input\"]\n",
    "print(pairs_df[\"Masked_Input\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1740740393021,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "4MHv8jfPv8wg",
    "outputId": "4cc51f63-7bed-4772-eabf-d88ba0286dba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Drug id  1EVU  1NSI  1DJL  1AB2  1ALS  1CFG  1EG0  1OZ5  4BSJ  ...  1TVB  \\\n",
      "0  DB11300     1     0     0     0     0     1     0     0     0  ...     0   \n",
      "1  DB11311     1     0     0     0     0     0     0     0     0  ...     0   \n",
      "2  DB11571     1     0     0     0     0     1     0     0     0  ...     0   \n",
      "3  DB13151     1     0     0     0     0     1     0     0     0  ...     0   \n",
      "4  DB05383     0     1     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1T5Q  6U6U  7RY7  2MDP  1G2C  4BPU  2X18  2N80  2KR6  \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0  \n",
      "3     0     0     0     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 721 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Load the Label Matrix ---\n",
    "# Ensure that the CSV file now has \"Drug id\" as header in the first column\n",
    "label_matrix = pd.read_csv(\"target_labels.csv\")\n",
    "print(label_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "puiWq7vgv8wg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DtiDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert sequence list back to string if needed\n",
    "        text = ' '.join(map(str, self.texts[idx])) if isinstance(self.texts[idx], list) else str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by tokenizer\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woeJjeOd4TAX"
   },
   "source": [
    "#### Load BERT Model and Optimizer\n",
    "Initialize the BERT tokenizer and the `BertForSequenceClassification` model for predicting drug-target interactions (using 2 labels for single classification). Set up the AdamW optimizer with a learning rate of 2e-5 to update model weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6O6WePl4QVW"
   },
   "source": [
    "# Create Dataset and DataLoader\n",
    "Extract the masked sequences and map each drug ID to its corresponding binary label (derived from the label matrix). Use these to create a PyTorch DataLoader, which facilitates efficient batch processing during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1740740829979,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "PopmsUE8v8wg",
    "outputId": "cb9665db-fe5f-4c3e-9e14-1dcf8182da1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 6275 103 103 2570 2603 2484 2321 2324 2539 2340 2260 1016 5139 5764 6163 1019 1020 2676 103 3590 1021 2654 3943 4090 2756 3486 103 2410 2403 4868 1023 103 4724 3963 6640 1018 2570 2603 2484 2321 2324 2539 2340 2260 1016 5139 5764 6163 1019 1020 4229 4464 103 103 4601 2410 2403 4868 1023 4720 4724 2184 1017 1018 5388 103 2184 2382 2538 2423 2538 5139 5764 6163 1019 1020 4229 4464 1021 2871 4601 2410 2403 4868 1023 4720 4724 4805 6273 1016 6255 2423 1018 2570 2603 2484 2321 103 2539 2340 103 1016 103 1016 6255 2423 1018 5388 5594 2184 2382 2538 15114 2184 103 2538 2423 102 8148 16327 1020 1016 100 103 28135 19348 6535 103 103 100 25256 8148 25578 16345 17317 12457 14993 4749 4720 6146 18744 103 9402 18512 22018 2423 18545 13029 16666 4868 7558 103 6255 24622 27234 15170 103 9402 12457 4029 2459 12862 20311 103 2538 15028 25504 103 19936 20713 13029 10550 9800 25586 21679 103 16798 11899 5388 6079 13427 19988 103 25103 15017 2382 9800 103 28489 25143 4700 14078 8574 12457 24331 15407 5354 6146 9800 103 103 6273 1017 24194 17273 19936 21777 20500 6275 17832 22884 18596 7287 2861 4700 103 23090 24232 15407 14078 3515 2382 19883 100 28906 5786 2531 17405 19348 6486 1021 3438 13285 16333 20549 6079 19289 6273 1022 103 2322 22376 21679 1023 2184 103 17273 1018 2184 1020 2260 103 25717 2753 5989 12963 8732 3515 4185 13427 20035 103 103 22234 103 100 24709 13092 103 16734 103 3263 2340 10373 5345 3116 16459 19492 5018 6391 5179 8148 12875 4601 12567 18582 103 6146 15170 22997 103 22997 19652 16065 103 22376 17825 24368 12875 11899 7558 100 100 5388 6079 6445 4008 13029 11118 5187 4868 2322 15471 10545 6282 2871 103 6163 14078 4229 16333 20549 6079 103 18582 103 18561 19871 5818 17943 10715 8148 27173 103 6535 5818 16798 103 5818 27502 100 19627 4293 17832 5345 21489 103 6079 12904 18610 22238 22343 17317 6227 13741 103 1019 100 103 24837 2322 26628 103 103 5139 2403 2656 6079 7287 2861 5401 6282 2871 18744 6036 103 103 2603 22312 103 2382 9800 16333 13285 16333 24030 27200 12104 2321 25371 23532 14989 22238 6445 15028 100 100 13092 2321 2410 3515 9402 5539 20003 6282 10114 2403 3438 22904 22431 103 17528 28358 103 103 24426 27908 23785 6390 12104 20636 29335 6337 24559 19988 8746 20713 3770 2753 16065 103 11176 27908 103 27003 24709 100 100 6191 6640 4261 18118 18596 19410 13285 25586 103 103 5187 20802 25103 15017 24921 25256 5818 17943 13138 5989 5354 3429 22997 29562 16444 23380 6391 4466 1019 13741 17403 102\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Create Dataset and DataLoader ---\n",
    "# Extract texts from the pairs DataFrame:\n",
    "texts = pairs_df[\"Masked_Input\"].tolist()\n",
    "print(texts[0])\n",
    "# For each drug id in pairs_df, get its label vector from the label_matrix.\n",
    "# Here we assume the label_matrix’s index matches the \"Drug id\" values.\n",
    "\n",
    "# Convert a multi-label vector into a single label (if that is your intended approach)\n",
    "\n",
    "labels = pairs_df[\"label\"].tolist()\n",
    "# print(label_matrix.head())\n",
    "#labels = pairs_df[\"Drug id\"].apply(lambda drug: label_matrix.loc[drug].values.tolist()).tolist()\n",
    "\n",
    "# Now create the dataset with the extracted lists\n",
    "print(labels[0])\n",
    "dataset = DtiDataset(texts, labels, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "# print(dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load BERT configuration\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # BERT base model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Paper-specific architecture\n",
    "        self.batch_norm1 = nn.BatchNorm1d(config.hidden_size)\n",
    "        \n",
    "        # First transformation block (768 -> 1024)\n",
    "        self.transform1 = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(1024)\n",
    "        )\n",
    "        \n",
    "        # Second transformation block (1024 -> 512)\n",
    "        self.transform2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(512)\n",
    "        )\n",
    "        \n",
    "        # Third transformation block (512 -> 256)\n",
    "        self.transform3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(256, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get pooled output and apply first batch norm\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.batch_norm1(pooled_output)\n",
    "        \n",
    "        # Apply transformation blocks\n",
    "        x = self.transform1(pooled_output)\n",
    "        x = self.transform2(x)\n",
    "        x = self.transform3(x)\n",
    "        \n",
    "        # Get logits\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.11/site-packages (7.6.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets) (6.28.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets) (5.9.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/anaconda3/lib/python3.11/site-packages (from nbformat>=4.2.0->ipywidgets) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/anaconda3/lib/python3.11/site-packages (from nbformat>=4.2.0->ipywidgets) (4.19.2)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (7.0.8)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.25.1)\n",
      "Requirement already satisfied: jupyterlab<4.1,>=4.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.3)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (7.10.0)\n",
      "Requirement already satisfied: overrides in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.58.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/anaconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.6)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/anaconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in /opt/anaconda3/lib/python3.11/site-packages (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2023.3.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.1.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2024.8.30)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/anaconda3/lib/python3.11/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.1)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.5)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.2.3)\n",
      "Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webcolors, uri-template, fqdn, isoduration\n",
      "Successfully installed fqdn-1.5.1 isoduration-20.11.0 uri-template-1.3.0 webcolors-24.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "executionInfo": {
     "elapsed": 17077,
     "status": "error",
     "timestamp": 1740740922759,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "wR59Bl9YLoOI",
    "outputId": "b1633108-b65e-4284-97b6-83962dab7356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loaded...\n",
      "Val Loaded...\n",
      "Test Loaded...\n",
      "Number of training samples: 1923\n",
      "Number of validation samples: 481\n",
      "Number of testing samples: 500\n",
      "Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Input IDs shape: torch.Size([64, 128])\n",
      "Attention mask shape: torch.Size([64, 128])\n",
      "Labels: tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ce95494ebf47eeb4befb0ea1612c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4124280402d14d1d8312e39c29ea8884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! (Validation Accuracy: 0.6133)\n",
      "\n",
      "Epoch 1 - Average loss: 0.7345 - Val loss: 0.6690 - Val accuracy: 0.6133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693284b583c74684afb9adfd913e81a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f4aed43f2145b09260e096855dd6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - Average loss: 0.7345 - Val loss: 0.6802 - Val accuracy: 0.5967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa8c0eb7e294e2bab8fdf27458eecaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a88b1550e84ab1b0697ed11797b59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 - Average loss: 0.7345 - Val loss: 0.7467 - Val accuracy: 0.3784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2a9e40e19b41ad82c24de001fcca7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    114\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[131], line 48\u001b[0m, in \u001b[0;36mCustomBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Get BERT outputs\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m     49\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     50\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Get pooled output and apply first batch norm\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1143\u001b[0m     embedding_output,\n\u001b[1;32m   1144\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1145\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1146\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1147\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1148\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1149\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1150\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1151\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    696\u001b[0m         hidden_states,\n\u001b[1;32m    697\u001b[0m         attention_mask,\n\u001b[1;32m    698\u001b[0m         layer_head_mask,\n\u001b[1;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    701\u001b[0m         past_key_value,\n\u001b[1;32m    702\u001b[0m         output_attentions,\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[1;32m    629\u001b[0m )\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import torch.optim as optim\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts[:-500], labels[:-500], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = texts[-500:]\n",
    "test_labels = labels[-500:]\n",
    "\n",
    "# -------------------------\n",
    "# Create Datasets and DataLoader\n",
    "# -------------------------\n",
    "\n",
    "# Optimizer with weight decay fix\n",
    "num_epochs = 5\n",
    "model = CustomBertModel(num_labels=2)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "train_dataset = DtiDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = DtiDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "test_dataset = DtiDataset(test_texts, test_labels, tokenizer, max_length=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "print(f\"Train Loaded...\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "print(f\"Val Loaded...\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "print(f\"Test Loaded...\")\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Evaluating')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs['loss']\n",
    "            total_loss += loss.item()\n",
    "            logits = outputs['logits']\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == batch['labels']).sum().item()\n",
    "            total += len(batch['labels'])\n",
    "            \n",
    "            current_accuracy = correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{total_loss/(progress_bar.n+1):.4f}',\n",
    "                'accuracy': f'{current_accuracy:.4f}'\n",
    "            })\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "# -------------------------\n",
    "# (Optional) Iterate over a batch from train_loader to inspect the data\n",
    "# -------------------------\n",
    "for batch in train_loader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Attention mask shape:\", batch[\"attention_mask\"].shape)\n",
    "    print(\"Labels:\", batch[\"labels\"])\n",
    "    break\n",
    "\n",
    "# -------------------------\n",
    "# Model Initialization and Training (Example)\n",
    "# -------------------------\n",
    "best_val_accuracy = 0\n",
    "\n",
    "## Training loop with progress bars\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Improved optimizer settings\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=2e-5,  # Reduced learning rate\n",
    "                       weight_decay=0.01,  # L2 regularization\n",
    "                       eps=1e-8)\n",
    "\n",
    "# Add learning rate scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = num_training_steps // 10  # 10% warmup\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Modified training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        logits = outputs['logits']\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == batch['labels']).sum().item()\n",
    "        total += len(batch['labels'])\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{total_loss/(progress_bar.n+1):.4f}',\n",
    "            'accuracy': f'{correct/total:.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "\n",
    "    # Evaluate and save\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader)\n",
    "    \n",
    "    # print(f\"\\nEpoch {epoch + 1}:\")\n",
    "    # print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    # print(f\"Train Accuracy: {correct/total:.4f}\")\n",
    "    # print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    # print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_accuracy,\n",
    "            'val_loss': val_loss\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"New best model saved! (Validation Accuracy: {val_accuracy:.4f})\")\n",
    "    print(f\"\\nEpoch {epoch + 1} - Average loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/kgkwlzhd025fqwv56bs8t87m0000gn/T/ipykernel_39670/2112172243.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomBertModel:\n\tMissing key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"batch_norm1.weight\", \"batch_norm1.bias\", \"batch_norm1.running_mean\", \"batch_norm1.running_var\", \"transform1.0.weight\", \"transform1.0.bias\", \"transform1.3.weight\", \"transform1.3.bias\", \"transform1.3.running_mean\", \"transform1.3.running_var\", \"transform2.0.weight\", \"transform2.0.bias\", \"transform2.3.weight\", \"transform2.3.bias\", \"transform2.3.running_mean\", \"transform2.3.running_var\", \"transform3.0.weight\", \"transform3.0.bias\", \"transform3.3.weight\", \"transform3.3.bias\", \"transform3.3.running_mean\", \"transform3.3.running_var\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"val_acc\", \"val_loss\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate model on test set\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     55\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m test(model, test_loader)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomBertModel:\n\tMissing key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"batch_norm1.weight\", \"batch_norm1.bias\", \"batch_norm1.running_mean\", \"batch_norm1.running_var\", \"transform1.0.weight\", \"transform1.0.bias\", \"transform1.3.weight\", \"transform1.3.bias\", \"transform1.3.running_mean\", \"transform1.3.running_var\", \"transform2.0.weight\", \"transform2.0.bias\", \"transform2.3.weight\", \"transform2.3.bias\", \"transform2.3.running_mean\", \"transform2.3.running_var\", \"transform3.0.weight\", \"transform3.0.bias\", \"transform3.3.weight\", \"transform3.3.bias\", \"transform3.3.running_mean\", \"transform3.3.running_var\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"val_acc\", \"val_loss\". "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Evaluating')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions and probabilities\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            # Store predictions, scores (probability of positive class), and labels\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_scores.extend(probs[:, 1].cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{total_loss/(progress_bar.n+1):.4f}',\n",
    "                'accuracy': f'{current_accuracy:.4f}'\n",
    "            })\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy_score(all_labels, all_predictions),\n",
    "        'precision': precision_score(all_labels, all_predictions, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_predictions, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_predictions, zero_division=0),\n",
    "        'auc_roc': roc_auc_score(all_labels, all_scores),\n",
    "        'auprc': average_precision_score(all_labels, all_scores)\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate model on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_metrics = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1740734532871,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "KJ4eg-4hv8wg",
    "outputId": "b1eae729-afe3-43ce-9495-8fa5318e12f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drug id', 'pdb_id', 'label', 'smiles', 'Encoded_x', 'sequence', 'Encoded_y', 'Masked_Input']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(pairs_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjpwHdCm33ad"
   },
   "source": [
    "### Model Evalutation\n",
    "Due to small dataset and overfitting, accuracy is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362287,
     "status": "ok",
     "timestamp": 1740738163424,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "zLs5DcS4v8wh",
    "outputId": "725c6686-4a99-49fe-f453-048c40e15ebb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 61/61 [06:02<00:00,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# model.eval()\n",
    "# correct = total = 0\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(train_loader, desc=\"Evaluating\"):\n",
    "#         outputs = model(**batch)\n",
    "#         preds = outputs.logits.argmax(dim=1)\n",
    "#         correct += (preds == batch['labels']).sum().item()\n",
    "#         total += len(batch['labels'])\n",
    "# accuracy = correct / total\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-N16coW3_vj"
   },
   "source": [
    "# Prediction Function\n",
    "Takes a drug ID and a PDB ID as inputs, constructs the BERT input sequence, tokenizes it, and uses the trained model to predict whether an interaction exists. The function returns \"yes\" or \"no\" based on the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1740738685279,
     "user": {
      "displayName": "Ishita Godani",
      "userId": "01539488657360779054"
     },
     "user_tz": -330
    },
    "id": "UicD2YXWv8wh",
    "outputId": "80ee173e-4e13-4a4d-80de-09e4a570cbf2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def predict_interaction(drug_id, pdb_id, tokenizer=tokenizer, model=model):\n",
    "#     input_str = f\"[CLS] {drug_id} [SEP] {pdb_id} [SEP]\"\n",
    "#     inputs = tokenizer(input_str, return_tensors='pt', truncation=True, max_length=512)\n",
    "#     outputs = model(**inputs)\n",
    "#     prediction = outputs.logits.argmax(dim=1).item()\n",
    "#     return \"yes\" if prediction == 1 else \"no\"\n",
    "# predict_interaction('DB01254','1GQ5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
