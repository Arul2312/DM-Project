{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA5xhvwsyAOt"
      },
      "source": [
        "# BERT Pipeline for Drug-Target Interaction (DTI) Prediction\n",
        "\n",
        "This notebook implements an end-to-end pipeline for predicting drug-target interactions using a BERT-based model. The workflow includes:\n",
        "\n",
        "- *Data Preprocessing:* Cleaning, merging, and filtering raw CSV datasets.\n",
        "- *Frequent Consecutive Subsequence (FCS) Extraction:* Extracting and ranking meaningful subsequences from drug SMILES and protein sequences.\n",
        "- *Sequence Encoding:* Converting sequences into token indices using generated token dictionaries.\n",
        "- *BERT Input Construction and Masking:* Creating unified BERT inputs with special tokens and applying dynamic masking.\n",
        "- *Model Training and Evaluation:* Training a BERT-based classifier and evaluating its performance using ROC-AUC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JX_Cybg2wSqf",
        "outputId": "c777c589-991a-49d2-8479-72c9d9180bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.15.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\neha desai\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\neha desai\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\neha desai\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\neha desai\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUXxVB0ydYU"
      },
      "source": [
        "##Importing required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 485,
      "metadata": {
        "id": "t5bw0HDDv8wQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm # For processing bars\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "metadata": {
        "id": "KWnFuIFoLAUX"
      },
      "outputs": [],
      "source": [
        "#Define file paths\n",
        "DRUGBANK_CSV = \"drug_smiles.csv\"\n",
        "PROTEIN_CSV = \"pdb_sequences.csv\"\n",
        "INTERACTIONS_CSV = \"confirmed_interactions.csv\"\n",
        "DRUG_ENCODED_CSV = \"drug_encoded.csv\"\n",
        "PROTEIN_ENCODED_CSV = \"protein_encoded.csv\"\n",
        "DRUG_FCS_CSV = \"drug_smiles_fcs_freq_100.csv\"\n",
        "PROTEIN_FCS_CSV = \"protein_fcs_freq_100.csv\"\n",
        "MERGED_ENCODING_CSV = \"merged_encodings.csv\"\n",
        "TARGET_LABELS_CSV = \"target_labels.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBiF18kZyiCk"
      },
      "source": [
        "###Frequent Consecutive Subsequence (FCS) Extraction\n",
        "\n",
        "This section extracts all consecutive subsequences (using a sliding window) from the input strings (drug SMILES and protein sequences). The extraction function also prunes low-frequency subsequences periodically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "metadata": {
        "id": "vUsfGha1v8wT"
      },
      "outputs": [],
      "source": [
        "def extract_fcs_subsequences_stream(strings, min_length=2, max_length=None,\n",
        "                                      prune_every=1000, prune_threshold=5,\n",
        "                                      preserve_short_length=5):\n",
        "    \"\"\"\n",
        "    Extract frequent consecutive subsequences from a list of strings in a streaming fashion.\n",
        "    Uses different pruning criteria: always keep subsequences shorter than `preserve_short_length`\n",
        "    while pruning longer ones based on prune_threshold.\n",
        "\n",
        "    :param strings: Iterable of strings (e.g., drug SMILES or protein sequences).\n",
        "    :param min_length: Minimum subsequence length.\n",
        "    :param max_length: Maximum subsequence length to consider (if None, use full length available).\n",
        "    :param prune_every: After processing this many sequences, prune the counter.\n",
        "    :param prune_threshold: For subsequences of length >= preserve_short_length,\n",
        "                            remove those with counts lower than this threshold during pruning.\n",
        "    :param preserve_short_length: Subsequences with length less than this value will be preserved regardless.\n",
        "    :return: A Counter mapping each subsequence to its frequency.\n",
        "    \"\"\"\n",
        "    subseq_counter = Counter()\n",
        "\n",
        "    for idx, s in enumerate(strings, 1):\n",
        "        n = len(s)\n",
        "        for i in range(n):\n",
        "            current_max = n - i if max_length is None else min(max_length, n - i)\n",
        "            for l in range(min_length, current_max + 1):\n",
        "                subseq = s[i:i+l]\n",
        "                subseq_counter[subseq] += 1\n",
        "\n",
        "        # Prune periodically to free memory:\n",
        "        if idx % prune_every == 0:\n",
        "            # For short subsequences (length < preserve_short_length), keep them always;\n",
        "            # for longer ones, only keep if count >= prune_threshold.\n",
        "            subseq_counter = Counter({\n",
        "                k: v for k, v in subseq_counter.items()\n",
        "                if len(k) < preserve_short_length or v >= prune_threshold\n",
        "            })\n",
        "            print(f\"Processed {idx} sequences, counter pruned to {len(subseq_counter)} keys.\")\n",
        "\n",
        "    return subseq_counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTYbmKKwzJwF"
      },
      "source": [
        "###Filtering and Ranking Subsequences\n",
        "\n",
        "We filter the extracted subsequences by applying a minimum frequency threshold and then rank them. The result is saved into a DataFrame containing each subsequence, its frequency, and rank.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 488,
      "metadata": {
        "id": "rute13mov8wU"
      },
      "outputs": [],
      "source": [
        "def filter_and_rank_fcs(fcs_counts, min_frequency=100):\n",
        "    \"\"\"\n",
        "    Filter and rank subsequences that appear with frequency >= min_frequency.\n",
        "\n",
        "    :param fcs_counts: Counter mapping subsequences to frequency.\n",
        "    :param min_frequency: Minimum frequency for a subsequence to be included.\n",
        "    :return: A pandas DataFrame with columns [Subsequence, Frequency, Rank].\n",
        "    \"\"\"\n",
        "    filtered_items = [(subseq, freq) for subseq, freq in fcs_counts.items() if freq >= min_frequency]\n",
        "    filtered_items.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    data = []\n",
        "    for rank, (subseq, freq) in enumerate(filtered_items, start=1):\n",
        "        data.append({\"Subsequence\": subseq, \"Frequency\": freq, \"Rank\": rank})\n",
        "\n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLH2LUtEFeCP",
        "outputId": "e1363720-c380-478d-fc33-a457c4df1a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drug_smiles_fcs_freq_100.csv already exists. Skipping drug SMILES subsequence extraction.\n",
            "protein_fcs_freq_100.csv already exists. Skipping protein sequences subsequence extraction.\n"
          ]
        }
      ],
      "source": [
        "drugbank_csv = DRUGBANK_CSV          # CSV file for drug SMILES\n",
        "smiles_column = \"smiles\"                  # Column name in drugbank.csv\n",
        "\n",
        "protein_csv = PROTEIN_CSV         # CSV file for protein sequences\n",
        "protein_column = \"sequence\"               # Column name in protein_sequences.csv\n",
        "\n",
        "# Parameters for subsequence extraction\n",
        "min_length = 2              # Generate subsequences of length 2 or more\n",
        "max_length = 10             # Maximum subsequence length (adjust as needed)\n",
        "min_freq_threshold = 5    # Final frequency threshold for inclusion\n",
        "\n",
        "# Pruning parameters:\n",
        "prune_every = 1000          # Prune every 1000 sequences processed\n",
        "prune_threshold = 5         # For subsequences with length >= preserve_short_length, prune if count < 5\n",
        "preserve_short_length = 5   # Always preserve subsequences with length < 5\n",
        "\n",
        "# --- PROCESS DRUG SMILES ---\n",
        "drug_output_file = DRUG_FCS_CSV #path defined\n",
        "if not os.path.exists(drug_output_file):\n",
        "    df_drug = pd.read_csv(drugbank_csv)\n",
        "    if smiles_column not in df_drug.columns:\n",
        "        raise ValueError(f\"No '{smiles_column}' column found in {drugbank_csv}!\")\n",
        "\n",
        "    smiles_list = df_drug[smiles_column].dropna().astype(str).tolist()\n",
        "    print(\"Extracting subsequences from drug SMILES...\")\n",
        "\n",
        "    smiles_fcs_counts = extract_fcs_subsequences_stream(\n",
        "        smiles_list,\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "        prune_every=prune_every,\n",
        "        prune_threshold=prune_threshold,\n",
        "        preserve_short_length=preserve_short_length\n",
        "    )\n",
        "    smiles_fcs_df = filter_and_rank_fcs(smiles_fcs_counts, min_frequency=min_freq_threshold)\n",
        "    smiles_fcs_df.to_csv(drug_output_file, index=False)\n",
        "\n",
        "    print(f\"[DRUG SMILES] Total subsequences (frequency ≥ {min_freq_threshold}): {len(smiles_fcs_df)}\")\n",
        "    print(smiles_fcs_df.head(10))\n",
        "else:\n",
        "    print(f\"{drug_output_file} already exists. Skipping drug SMILES subsequence extraction.\")\n",
        "\n",
        "# --- PROCESS PROTEIN SEQUENCES ---\n",
        "protein_output_file = PROTEIN_FCS_CSV #path defined\n",
        "if not os.path.exists(protein_output_file):\n",
        "    df_protein = pd.read_csv(protein_csv)\n",
        "    if protein_column not in df_protein.columns:\n",
        "        raise ValueError(f\"No '{protein_column}' column found in {protein_csv}!\")\n",
        "\n",
        "    protein_list = df_protein[protein_column].dropna().astype(str).tolist()\n",
        "    print(\"Extracting subsequences from protein sequences...\")\n",
        "\n",
        "    protein_fcs_counts = extract_fcs_subsequences_stream(\n",
        "        protein_list,\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "        prune_every=prune_every,\n",
        "        prune_threshold=prune_threshold,\n",
        "        preserve_short_length=preserve_short_length\n",
        "    )\n",
        "    protein_fcs_df = filter_and_rank_fcs(protein_fcs_counts, min_frequency=min_freq_threshold)\n",
        "    protein_fcs_df.to_csv(protein_output_file, index=False)\n",
        "\n",
        "    print(f\"[PROTEIN SEQUENCES] Total subsequences (frequency ≥ {min_freq_threshold}): {len(protein_fcs_df)}\")\n",
        "    print(protein_fcs_df.head(10))\n",
        "else:\n",
        "    print(f\"{protein_output_file} already exists. Skipping protein sequences subsequence extraction.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH1UhP4NzdIc"
      },
      "source": [
        "###Sequence Encoding\n",
        "\n",
        "In this section, we encode each sequence (drug SMILES or protein sequence) by mapping every valid subsequence to its corresponding token index using a token dictionary. If a subsequence is missing in the dictionary, a default value of 0 is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {
        "id": "yT2LoPdUv8wW"
      },
      "outputs": [],
      "source": [
        "def encode_sequence(seq, token_dict, min_subseq_len=2):\n",
        "    \"\"\"\n",
        "    Encodes a sequence (drug SMILES or protein sequence) by scanning all consecutive subsequences\n",
        "    of length >= min_subseq_len and mapping them to their token indices (or 0 if not found).\n",
        "    \"\"\"\n",
        "    return [token_dict.get(seq[i:j], 0)\n",
        "            for i in range(len(seq))\n",
        "            for j in range(i + min_subseq_len, len(seq) + 1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oeYBmZEzatx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "metadata": {
        "id": "p2untJhXv8wW"
      },
      "outputs": [],
      "source": [
        "drug_df = pd.read_csv(DRUGBANK_CSV)             # Must contain columns \"Drug id\" and \"smiles\"\n",
        "protein_df = pd.read_csv(PROTEIN_CSV)  # Must contain columns \"pbd id\" and \"sequence\"\n",
        "\n",
        "# Load token dictionaries for drugs and proteins\n",
        "drug_dict_df = pd.read_csv(DRUG_FCS_CSV)    # Columns: \"Subsequence\", \"Rank\"\n",
        "protein_dict_df = pd.read_csv(PROTEIN_FCS_CSV)       # Columns: \"Subsequence\", \"Rank\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX8G4nNQJOlR",
        "outputId": "450b69ef-a3fd-4d02-92b7-94cf9f8eb62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drug_encoded.csv and protein_encoded.csv already exist. Skipping encoding step.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Only run encoding if either output file doesn't exist.\n",
        "if not os.path.exists(DRUG_ENCODED_CSV) or not os.path.exists(PROTEIN_ENCODED_CSV):\n",
        "    # Create Python dictionaries for mapping\n",
        "    drug_token_dict = dict(zip(drug_dict_df[\"Subsequence\"], drug_dict_df[\"Rank\"]))\n",
        "    protein_token_dict = dict(zip(protein_dict_df[\"Subsequence\"], protein_dict_df[\"Rank\"]))\n",
        "\n",
        "    # Encode drug SMILES and protein sequences separately.\n",
        "    # You can adjust min_subseq_len if needed.\n",
        "    drug_df[\"Encoded\"] = drug_df[\"smiles\"].apply(lambda x: encode_sequence(x, drug_token_dict, min_subseq_len=2))\n",
        "    protein_df[\"Encoded\"] = protein_df[\"sequence\"].apply(lambda x: encode_sequence(x, protein_token_dict, min_subseq_len=2))\n",
        "\n",
        "    # print(\"Encoded drug SMILES:\")\n",
        "    # print(drug_df[[\"Drug id\", \"Encoded\"]].head())\n",
        "    # print(\"Encoded protein sequences:\")\n",
        "    # print(protein_df[[\"pdb_id\", \"Encoded\"]].head())\n",
        "\n",
        "    # Save the intermediate results to CSV for further inspection.\n",
        "    drug_df.to_csv(DRUG_ENCODED_CSV, index=False)\n",
        "    protein_df.to_csv(PROTEIN_ENCODED_CSV, index=False)\n",
        "else:\n",
        "    print(f\"{DRUG_ENCODED_CSV} and {PROTEIN_ENCODED_CSV} already exist. Skipping encoding step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AOtfvdMx8y1"
      },
      "source": [
        "TRIAL!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqlrZcLQx8QA",
        "outputId": "c5f41764-892d-4e83-8bdb-2649f15f5538"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Neha Desai\\AppData\\Local\\Temp\\ipykernel_11992\\491441502.py:2: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  target_labels = pd.read_csv(TARGET_LABELS_CSV, header=None)\n"
          ]
        }
      ],
      "source": [
        "# Load target_labels (binary matrix with no headers)\n",
        "target_labels = pd.read_csv(TARGET_LABELS_CSV, header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {
        "id": "hFWlT1NmyI3d"
      },
      "outputs": [],
      "source": [
        "# Extract drug IDs (first column) and protein IDs (first row)\n",
        "drug_ids = target_labels.iloc[1:, 0].values  # Skip the first row\n",
        "protein_ids = target_labels.iloc[0, 1:].values  # Skip the first column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {
        "id": "cWjT75TVyLbI"
      },
      "outputs": [],
      "source": [
        "# Convert the binary matrix into a DataFrame with proper row/col names\n",
        "interaction_matrix = target_labels.iloc[1:, 1:].astype(int).values\n",
        "interaction_df = pd.DataFrame(interaction_matrix, index=drug_ids, columns=protein_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSpC7SSxyTkL"
      },
      "source": [
        "RANDOM SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {
        "id": "V1TiMJviyP-K"
      },
      "outputs": [],
      "source": [
        "# Get all (drug, protein) pairs where interaction = 1 (positive samples)\n",
        "positive_pairs = np.argwhere(interaction_matrix == 1)\n",
        "negative_pairs = np.argwhere(interaction_matrix == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {
        "id": "PL25chtlydd4"
      },
      "outputs": [],
      "source": [
        "positive_samples = [(str(drug_ids[i]), str(protein_ids[j])) for i, j in positive_pairs]\n",
        "negative_samples = [(str(drug_ids[i]), str(protein_ids[j])) for i, j in negative_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {
        "id": "bhtXrPhHkI1g"
      },
      "outputs": [],
      "source": [
        "# Randomly sample 500 positives and 500 negatives\n",
        "positive_sampled = np.random.choice(len(positive_samples), 500, replace=False)\n",
        "negative_sampled = np.random.choice(len(negative_samples), 500, replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {
        "id": "x3m_a2G2kJ7d"
      },
      "outputs": [],
      "source": [
        "positive_samples = [positive_samples[i] for i in positive_sampled]\n",
        "negative_samples = [negative_samples[i] for i in negative_sampled]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {
        "id": "But1hs9mygvt"
      },
      "outputs": [],
      "source": [
        "# Sample an equal number of negative samples as positives\n",
        "#num_pos = len(positive_samples)\n",
        "#negative_indices = np.random.choice(len(negative_samples), num_pos, replace=False)\n",
        "#negative_samples = [negative_samples[i] for i in negative_indices]  # Extract (Drug_ID, Protein_ID) pairs properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 501,
      "metadata": {
        "id": "MUwF-C-lkWmo"
      },
      "outputs": [],
      "source": [
        "df_selected = pd.DataFrame(positive_samples + negative_samples, columns=[\"Drug id\", \"pdb_id\"])\n",
        "df_selected[\"Interaction\"] = [1] * 500 + [0] * 500  # Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {
        "id": "ElyrEKpOv8wY"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Combine Encoded Sequences into BERT Input Format ---\n",
        "# Assume you have already encoded sequences stored in separate CSV files.\n",
        "# Here, we load them. We assume that the encoding is stored as a JSON-encoded list in a column.\n",
        "# If they are stored as space-separated strings, adjust the parsing accordingly.\n",
        "\n",
        "drug_encoded_df = pd.read_csv(DRUG_ENCODED_CSV)       # Columns: \"Drug id\", \"Encoded\"\n",
        "protein_encoded_df = pd.read_csv(PROTEIN_ENCODED_CSV)   # Columns: \"pbd id\", \"Encoded\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {
        "id": "63nVTqmmF5Il"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded Proteins:\n",
            "   pdb_id                                            Encoded\n",
            "0   1NSI  [289, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
            "1   1DJL  [307, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
            "2   1AB2  [45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
            "3   4BSJ  [292, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
            "4   1FLT  [21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
            "Encoded Drugs:\n",
            "    Drug id                                            Encoded\n",
            "0  DB00131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  DB00140  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "2  DB00148  [78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
            "3  DB00159  [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "4  DB00182  [44, 47, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n"
          ]
        }
      ],
      "source": [
        "print(\"Encoded Proteins:\\n\",protein_encoded_df[[\"pdb_id\", \"Encoded\"]].head())\n",
        "print(\"Encoded Drugs:\\n\",drug_encoded_df[[\"Drug id\",\"Encoded\"]].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {
        "id": "YK64PBzzv8wY"
      },
      "outputs": [],
      "source": [
        "# Convert the encoded column from string to list. Here we assume it's stored as JSON.\n",
        "drug_encoded_df[\"Encoded\"] = drug_encoded_df[\"Encoded\"].apply(json.loads)\n",
        "protein_encoded_df[\"Encoded\"] = protein_encoded_df[\"Encoded\"].apply(json.loads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "metadata": {
        "id": "8OVkZAkriNfM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered df_selected shape: (327, 3)\n"
          ]
        }
      ],
      "source": [
        "df_filtered = df_selected[\n",
        "    df_selected[\"Drug id\"].isin(drug_encoded_df[\"Drug id\"]) &\n",
        "    df_selected[\"pdb_id\"].isin(protein_encoded_df[\"pdb_id\"])\n",
        "]\n",
        "print(f\"Filtered df_selected shape: {df_filtered.shape}\")  # Should be much smaller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before merging, df_filtered labels: Counter({1: 252, 0: 75})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "print(\"Before merging, df_filtered labels:\", Counter(df_filtered[\"Interaction\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original selected pairs: Counter({1: 500, 0: 500})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Original selected pairs:\", Counter(df_selected[\"Interaction\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filtering df_selected: Counter({1: 252, 0: 75})\n"
          ]
        }
      ],
      "source": [
        "print(\"After filtering df_selected:\", Counter(df_filtered[\"Interaction\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {
        "id": "wUoNzOimzTw9"
      },
      "outputs": [],
      "source": [
        "merged_df = (\n",
        "    df_filtered\n",
        "    .merge(drug_encoded_df, on=\"Drug id\", how=\"inner\")\n",
        "    .merge(protein_encoded_df, on=\"pdb_id\", how=\"inner\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {
        "id": "zDsvJYtIlhLJ"
      },
      "outputs": [],
      "source": [
        "# Save merged dataset\n",
        "merged_df.to_csv(MERGED_ENCODING_CSV, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {
        "id": "HClMrJbBlhnT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final merged dataset saved as merged_encodings.csv.\n",
            "Interaction\n",
            "1    252\n",
            "0     75\n",
            "Name: count, dtype: int64\n",
            "   Drug id pdb_id  Interaction  \\\n",
            "0  DB06589   1GQ5            1   \n",
            "1  DB12147   1AGW            1   \n",
            "2  DB01331   2UWX            1   \n",
            "3  DB00308   1T0J            1   \n",
            "4  DB12598   1SPJ            1   \n",
            "\n",
            "                                              smiles  \\\n",
            "0  Cc1ccc(Nc2nccc(N(C)c3ccc4c(C)n(C)nc4c3)n2)cc1S...   \n",
            "1  COc1cc(OC)cc(N(CCNC(C)C)c2ccc3ncc(-c4cnn(C)c4)...   \n",
            "2  CO[C@@]1(NC(=O)Cc2cccs2)C(=O)N2C(C(=O)O)=C(COC...   \n",
            "3       CCCCCCCN(CC)CCC[C@H](O)c1ccc(NS(C)(=O)=O)cc1   \n",
            "4      N=C(N)c1ccc2cc(OC(=O)c3ccc(N=C(N)N)cc3)ccc2c1   \n",
            "\n",
            "                                           Encoded_x  \\\n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "2  [81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "3  [3, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                            sequence  \\\n",
            "0  GMLPRLCCLEKGPNGYGFHLHGEKGKLGQYIRLVEPGSPAEKAGLL...   \n",
            "1  ELPEDPRWELPRDRLVLGKPLGQVVLAEAIGLPNRVTKVAVKMLKS...   \n",
            "2  ISEITYSDGTVIASIDYLYFTTLAEAQERMYDYLAQRDNVSAKELK...   \n",
            "3  RREAERQAQAQLEKAKTKPVAFAVRTNVRYSAAQEDDVPVPGMAIS...   \n",
            "4  IVGGWECEQHSQPWQAALYHFSTFQCGGILVHRQWVLTAAHCISDN...   \n",
            "\n",
            "                                           Encoded_y  \n",
            "0  [245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
            "1  [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "2  [80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
            "3  [150, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
            "4  [82, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n"
          ]
        }
      ],
      "source": [
        "print(f\"Final merged dataset saved as {MERGED_ENCODING_CSV}.\")\n",
        "print(merged_df[\"Interaction\"].value_counts())  # Should show counts for both 1 and 0\n",
        "print(merged_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before filtering: Interaction\n",
            "1    500\n",
            "0    500\n",
            "Name: count, dtype: int64\n",
            "After filtering: Interaction\n",
            "1    252\n",
            "0     75\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Before filtering:\", df_selected[\"Interaction\"].value_counts())\n",
        "print(\"After filtering:\", df_filtered[\"Interaction\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before merging: Interaction\n",
            "1    252\n",
            "0     75\n",
            "Name: count, dtype: int64\n",
            "After merging: Interaction\n",
            "1    252\n",
            "0     75\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Before merging:\", df_filtered[\"Interaction\"].value_counts())\n",
        "print(\"After merging:\", merged_df[\"Interaction\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Drug id pdb_id  Interaction  \\\n",
            "0  DB06589   1GQ5            1   \n",
            "1  DB12147   1AGW            1   \n",
            "2  DB01331   2UWX            1   \n",
            "3  DB00308   1T0J            1   \n",
            "4  DB12598   1SPJ            1   \n",
            "\n",
            "                                              smiles  \\\n",
            "0  Cc1ccc(Nc2nccc(N(C)c3ccc4c(C)n(C)nc4c3)n2)cc1S...   \n",
            "1  COc1cc(OC)cc(N(CCNC(C)C)c2ccc3ncc(-c4cnn(C)c4)...   \n",
            "2  CO[C@@]1(NC(=O)Cc2cccs2)C(=O)N2C(C(=O)O)=C(COC...   \n",
            "3       CCCCCCCN(CC)CCC[C@H](O)c1ccc(NS(C)(=O)=O)cc1   \n",
            "4      N=C(N)c1ccc2cc(OC(=O)c3ccc(N=C(N)N)cc3)ccc2c1   \n",
            "\n",
            "                                           Encoded_x  \\\n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "2  [81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "3  [3, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                            sequence  \\\n",
            "0  GMLPRLCCLEKGPNGYGFHLHGEKGKLGQYIRLVEPGSPAEKAGLL...   \n",
            "1  ELPEDPRWELPRDRLVLGKPLGQVVLAEAIGLPNRVTKVAVKMLKS...   \n",
            "2  ISEITYSDGTVIASIDYLYFTTLAEAQERMYDYLAQRDNVSAKELK...   \n",
            "3  RREAERQAQAQLEKAKTKPVAFAVRTNVRYSAAQEDDVPVPGMAIS...   \n",
            "4  IVGGWECEQHSQPWQAALYHFSTFQCGGILVHRQWVLTAAHCISDN...   \n",
            "\n",
            "                                           Encoded_y  \n",
            "0  [245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
            "1  [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "2  [80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
            "3  [150, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
            "4  [82, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n"
          ]
        }
      ],
      "source": [
        "print(merged_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels: [1 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"Unique labels:\", merged_df[\"Interaction\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPucdr3qv8wZ",
        "outputId": "1d0f30a1-cdc6-4463-96c5-72ace9145f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File merged_encodings.csv already exists. Skipping save.\n"
          ]
        }
      ],
      "source": [
        "# Save the final merged file\n",
        "if not os.path.exists(MERGED_ENCODING_CSV):\n",
        "    merged_df.to_csv(MERGED_ENCODING_CSV, index=False)\n",
        "    print(f\"File {MERGED_ENCODING_CSV} created.\")\n",
        "else:\n",
        "    print(f\"File {MERGED_ENCODING_CSV} already exists. Skipping save.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {
        "id": "4C8jDn54v8wa"
      },
      "outputs": [],
      "source": [
        "# Define special token IDs\n",
        "CLS_TOKEN = \"[CLS]\"\n",
        "SEP_TOKEN = \"[SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {
        "id": "zr_c0zRyv8wa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1974: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "token = \"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\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_auth_token=token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {
        "id": "Obobg9pSv8wa"
      },
      "outputs": [],
      "source": [
        "# Initialize DataCollator for masking\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {
        "id": "MwRB84zev8wb"
      },
      "outputs": [],
      "source": [
        "def create_bert_input(drug_tokens, protein_tokens):\n",
        "    \"\"\"\n",
        "    Constructs the final BERT input sequence as:\n",
        "      [CLS] + drug_tokens + [SEP] + protein_tokens + [SEP]\n",
        "    \"\"\"\n",
        "    drug_str = \" \".join(map(str, drug_tokens))\n",
        "    protein_str = \" \".join(map(str, protein_tokens))\n",
        "    return f\"{CLS_TOKEN} {drug_str} {SEP_TOKEN} {protein_str} {SEP_TOKEN}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "id": "9-frHZlmv8wb"
      },
      "outputs": [],
      "source": [
        "def process_chunk(chunk):\n",
        "    # Convert encoded columns from JSON strings to lists if necessary\n",
        "    chunk[\"Encoded_x\"] = chunk[\"Encoded_x\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
        "    chunk[\"Encoded_y\"] = chunk[\"Encoded_y\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Create BERT input sequences: [CLS] drug_tokens [SEP] protein_tokens [SEP]\n",
        "    chunk[\"BERT_Input\"] = chunk.apply(lambda row: create_bert_input(row[\"Encoded_x\"], row[\"Encoded_y\"]), axis=1)\n",
        "\n",
        "    # Tokenize the input sequences with a maximum length of 512 tokens\n",
        "    tokenized = tokenizer(\n",
        "        chunk[\"BERT_Input\"].tolist(),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Convert the tokenized tensor to a list of dicts so that each element has an \"input_ids\" key.\n",
        "    features = [{\"input_ids\": ids} for ids in tokenized[\"input_ids\"].tolist()]\n",
        "\n",
        "    # Apply masking using the data collator.\n",
        "    masked = data_collator(features)\n",
        "\n",
        "    # Convert the masked output (a tensor) back to a space-separated string for storage.\n",
        "    chunk[\"Masked_Input\"] = [\" \".join(map(str, seq)) for seq in masked[\"input_ids\"].tolist()]\n",
        "\n",
        "    return chunk[[\"Masked_Input\", \"Interaction\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61s0IQzqv8wb",
        "outputId": "10a965e2-6f73-484b-c862-ffbb5613bfce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'final_bert_inputs_masked.h5' already exists. Skipping processing.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if not os.path.exists('final_bert_inputs_masked.h5'):\n",
        "    chunksize = 1000  # Adjust this based on your available memory\n",
        "\n",
        "    # Process and write output\n",
        "    with pd.HDFStore('final_bert_inputs_masked.h5', mode='w') as store:\n",
        "        for i, chunk in enumerate(pd.read_csv(\"merged_encodings.csv\", chunksize=chunksize)):\n",
        "            processed_chunk = process_chunk(chunk)\n",
        "            store.append('df', processed_chunk, format='table', data_columns=True)\n",
        "            print(f\"Processed chunk {i+1}\")\n",
        "\n",
        "    print(\"Final BERT input sequences with masking saved to final_bert_inputs_masked.h5\")\n",
        "else:\n",
        "    print(\"File 'final_bert_inputs_masked.h5' already exists. Skipping processing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed Data Shape: (279, 2)\n",
            "Processed Data Columns: Index(['Masked_Input', 'Interaction'], dtype='object')\n",
            "Label Distribution After Processing: Counter({1: 217, 0: 62})\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Read processed data from HDF5\n",
        "with pd.HDFStore('final_bert_inputs_masked.h5', mode='r') as store:\n",
        "    processed_data = store['df']  # Load the dataframe\n",
        "\n",
        "# Check if Interaction labels exist\n",
        "print(\"Processed Data Shape:\", processed_data.shape)\n",
        "print(\"Processed Data Columns:\", processed_data.columns)\n",
        "print(\"Label Distribution After Processing:\", Counter(processed_data[\"Interaction\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sucj1bx83mPe"
      },
      "source": [
        "###Model Training\n",
        "Training the BERT model by iterating over the DataLoader for a set number of epochs.\n",
        "Before training we are optimizing the input texts and labels to pass through Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIFxvvYov8wc",
        "outputId": "c2b49d28-47ed-4160-db52-257a4f6af931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\neha desai\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from h5py) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {
        "id": "tYY-2Q7yv8wc"
      },
      "outputs": [],
      "source": [
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X83SSDOjv8wc",
        "outputId": "e78df6c0-bbb9-434f-c7a0-2f663fa16d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HDF5 file structure:\n",
            "df: <class 'h5py._hl.group.Group'>\n",
            "  _i_table: <class 'h5py._hl.group.Group'>\n",
            "    Interaction: <class 'h5py._hl.group.Group'>\n",
            "      abounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      bounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      indices: <class 'h5py._hl.dataset.Dataset'>\n",
            "      indicesLR: <class 'h5py._hl.dataset.Dataset'>\n",
            "      mbounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      mranges: <class 'h5py._hl.dataset.Dataset'>\n",
            "      ranges: <class 'h5py._hl.dataset.Dataset'>\n",
            "      sorted: <class 'h5py._hl.dataset.Dataset'>\n",
            "      sortedLR: <class 'h5py._hl.dataset.Dataset'>\n",
            "      zbounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "    Masked_Input: <class 'h5py._hl.group.Group'>\n",
            "      abounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      bounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      indices: <class 'h5py._hl.dataset.Dataset'>\n",
            "      indicesLR: <class 'h5py._hl.dataset.Dataset'>\n",
            "      mbounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      mranges: <class 'h5py._hl.dataset.Dataset'>\n",
            "      ranges: <class 'h5py._hl.dataset.Dataset'>\n",
            "      sorted: <class 'h5py._hl.dataset.Dataset'>\n",
            "      sortedLR: <class 'h5py._hl.dataset.Dataset'>\n",
            "      zbounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "    index: <class 'h5py._hl.group.Group'>\n",
            "      abounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      bounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      indices: <class 'h5py._hl.dataset.Dataset'>\n",
            "      indicesLR: <class 'h5py._hl.dataset.Dataset'>\n",
            "      mbounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "      mranges: <class 'h5py._hl.dataset.Dataset'>\n",
            "      ranges: <class 'h5py._hl.dataset.Dataset'>\n",
            "      sorted: <class 'h5py._hl.dataset.Dataset'>\n",
            "      sortedLR: <class 'h5py._hl.dataset.Dataset'>\n",
            "      zbounds: <class 'h5py._hl.dataset.Dataset'>\n",
            "  table: <class 'h5py._hl.dataset.Dataset'>\n"
          ]
        }
      ],
      "source": [
        "def print_hdf5_structure(obj, indent=0):\n",
        "    for key in obj.keys():\n",
        "        item = obj[key]\n",
        "        print(\"  \" * indent + f\"{key}: {type(item)}\")\n",
        "        if isinstance(item, h5py.Group):\n",
        "            print_hdf5_structure(item, indent+1)\n",
        "\n",
        "with h5py.File('final_bert_inputs_masked.h5', 'r') as f:\n",
        "    print(\"HDF5 file structure:\")\n",
        "    print_hdf5_structure(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyigAOuv8wd",
        "outputId": "fa30493d-b7d0-4746-da7f-1452ad1d22f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 entries:\n",
            "[(0, b'101 101 1017 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 19628 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 6387 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 103 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 16232 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 2184 2382 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 2538 1014 1014 1014 22318 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 6881 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 27591 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 9699 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 6796 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 102', 1)\n",
            " (1, b'101 101 1017 1014 103 1014 103 1014 1014 1014 1014 1014 103 103 103 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 103 1014 6282 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 3438 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1018 2570 2603 2484 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 103 2324 2539 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 2260 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 22766 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 103 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1016 6255 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 2423 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 103 103 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 5401 1014 103 1014 17953 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1018 5388 5594 1014 1014 10767 1014 1014 1014 23145 103 1014 1014 1014 102', 1)\n",
            " (2, b'101 101 6275 103 103 1014 103 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 6640 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 14155 2570 2603 2484 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2321 2324 2539 1014 103 1014 22054 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2340 2260 1014 1014 28865 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1016 5786 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 2385 3770 1014 1014 1014 2907 1014 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 18214 1014 1014 1014 2459 4749 4583 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 3603 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 4413 103 1014 1014 103 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 13526 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1015 3429 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 13560 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 102', 1)\n",
            " (3, b'101 101 1017 1014 103 103 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 17800 6275 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 22442 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 103 2184 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1017 103 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 3744 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 28510 1014 1014 1014 1014 1014 1014 1014 2423 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1017 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1850 3910 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 6275 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 103 1014 6640 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 27523 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 376 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1018 2570 2603 2484 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 102', 1)\n",
            " (4, b'101 101 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 3511 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 4333 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1019 1020 4229 4464 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 441 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1021 2871 4601 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 103 1014 103 103 1014 103 1014 1014 1014 1014 1014 5013 1014 1014 1014 1014 1014 1014 1014 1014 2410 2403 4868 1014 1014 103 1014 103 1014 1014 1014 29368 103 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1023 4720 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 4724 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 103 1014 2184 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1018 2570 2603 2484 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 2321 2324 2539 1014 1014 1014 1014 103 1014 103 1014 1014 2276 1014 103 102', 1)\n",
            " (5, b'101 101 1017 2753 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 7125 1014 103 1014 1014 1014 1014 1017 1014 1014 1014 103 1014 1014 1014 1014 1014 103 103 1014 28239 1014 103 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 6282 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 3438 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1018 5388 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 103 2184 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1018 103 2603 2484 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 2321 2324 2539 6070 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 13221 1014 4580 1963 103 1014 1014 1014 103 2260 6584 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1016 6445 103 1014 1014 103 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 8005 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 6079 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 103 19183 1014 1014 1014 1014 1014 1014 1014 1014 1017 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 102', 1)\n",
            " (6, b'101 101 1017 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 6282 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 103 3438 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 103 1014 103 1014 1014 1014 6403 1014 1014 1014 1014 1014 1014 1018 2570 2603 2484 1014 1014 103 1014 103 1014 103 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 7739 1014 1014 1014 1014 1014 1014 1014 1014 1014 2321 2324 2539 1014 1014 7984 1014 1014 103 1014 1014 1014 103 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 2340 2260 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1016 5786 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 2385 3770 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 103 1014 2459 4749 4583 5824 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 13166 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 4413 5187 6421 1014 1014 1014 1014 16802 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1015 1022 6353 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 3429 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 103 1014 1014 103 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2322 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2184 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 102', 1)\n",
            " (7, b'101 101 4008 4700 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1019 1020 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 27180 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1021 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 13418 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 103 1014 1014 103 1014 103 29607 1014 1014 1014 1014 7100 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 9163 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 5179 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 24031 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 103 1014 1014 1014 103 103 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 103 1014 1014 1014 1017 1014 103 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 4008 4700 4466 1014 1014 1014 1014 1014 1014 102', 1)\n",
            " (8, b'101 101 6275 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 103 1014 678 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 103 1014 6640 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 19464 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 103 103 1014 1014 1014 1014 1014 1014 103 29310 1014 1014 1014 1014 1014 1014 1014 1014 1014 1018 2570 103 2484 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 103 1014 1014 1014 103 1014 1014 1014 4271 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 2321 2324 2539 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 14668 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 3395 1014 1014 1014 1014 1014 1014 1014 1014 2340 2260 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 10884 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 23879 1014 23184 103 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1016 5786 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 16663 1014 1014 1014 1014 1014 103 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2385 3770 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 2459 4749 4583 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 28833 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 4413 5187 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 14340 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1015 3429 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 102', 1)\n",
            " (9, b'101 101 1017 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 28933 1014 18030 1014 1014 1014 1014 1014 4008 4700 103 1014 103 1014 1014 1014 1014 18758 1014 1014 103 1014 1014 1014 12236 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 5284 1014 1014 1014 1014 1014 1014 103 1020 2676 2861 3590 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1021 103 3943 4090 1014 103 1014 1014 7533 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 2756 103 4029 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 2410 2403 6535 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1023 6146 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 5179 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 6640 1014 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 2570 2603 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 12841 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 2321 2324 2539 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 2340 2260 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1016 5786 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 5602 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2385 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 2656 4185 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 102', 1)]\n"
          ]
        }
      ],
      "source": [
        "with h5py.File('final_bert_inputs_masked.h5', 'r') as f:\n",
        "    # Access the final table dataset\n",
        "    dataset = f[\"df\"][\"table\"]\n",
        "\n",
        "    # Print the first 10 entries\n",
        "    print(\"First 10 entries:\")\n",
        "    print(dataset[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81875UVRv8wd",
        "outputId": "6f579d6c-0a32-4170-9791-8597bc72b7e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of entries: 279\n"
          ]
        }
      ],
      "source": [
        "with h5py.File('final_bert_inputs_masked.h5', 'r') as f:\n",
        "    dataset = f[\"df\"][\"table\"]\n",
        "    num_entries = dataset.shape[0]\n",
        "    print(\"Number of entries:\", num_entries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "metadata": {
        "id": "vM1nOofjv8wf"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Load Masked Inputs from HDF5 and Pair IDs from CSV ---\n",
        "masked_df = pd.read_hdf(\"final_bert_inputs_masked.h5\", key=\"df/table\")\n",
        "pairs_df = pd.read_csv(\"merged_encodings.csv\")  # Should include columns \"Drug id\" and \"pbd id\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR5_LZtSv8wf",
        "outputId": "64db9eba-8880-49a1-8b11-826da12ce4a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Drug id pdb_id  Interaction  \\\n",
            "0  DB06589   1GQ5            1   \n",
            "1  DB12147   1AGW            1   \n",
            "2  DB01331   2UWX            1   \n",
            "3  DB00308   1T0J            1   \n",
            "4  DB12598   1SPJ            1   \n",
            "\n",
            "                                              smiles  \\\n",
            "0  Cc1ccc(Nc2nccc(N(C)c3ccc4c(C)n(C)nc4c3)n2)cc1S...   \n",
            "1  COc1cc(OC)cc(N(CCNC(C)C)c2ccc3ncc(-c4cnn(C)c4)...   \n",
            "2  CO[C@@]1(NC(=O)Cc2cccs2)C(=O)N2C(C(=O)O)=C(COC...   \n",
            "3       CCCCCCCN(CC)CCC[C@H](O)c1ccc(NS(C)(=O)=O)cc1   \n",
            "4      N=C(N)c1ccc2cc(OC(=O)c3ccc(N=C(N)N)cc3)ccc2c1   \n",
            "\n",
            "                                           Encoded_x  \\\n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "2  [81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "3  [3, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                            sequence  \\\n",
            "0  GMLPRLCCLEKGPNGYGFHLHGEKGKLGQYIRLVEPGSPAEKAGLL...   \n",
            "1  ELPEDPRWELPRDRLVLGKPLGQVVLAEAIGLPNRVTKVAVKMLKS...   \n",
            "2  ISEITYSDGTVIASIDYLYFTTLAEAQERMYDYLAQRDNVSAKELK...   \n",
            "3  RREAERQAQAQLEKAKTKPVAFAVRTNVRYSAAQEDDVPVPGMAIS...   \n",
            "4  IVGGWECEQHSQPWQAALYHFSTFQCGGILVHRQWVLTAAHCISDN...   \n",
            "\n",
            "                                           Encoded_y  \\\n",
            "0  [245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
            "1  [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2  [80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "3  [150, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
            "4  [82, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "\n",
            "                                        Masked_Input  \n",
            "0  101 101 1017 1014 1014 1014 1014 1014 1014 101...  \n",
            "1  101 101 1017 1014 103 1014 103 1014 1014 1014 ...  \n",
            "2  101 101 6275 103 103 1014 103 103 103 1014 101...  \n",
            "3  101 101 1017 1014 103 103 1014 1014 1014 1014 ...  \n",
            "4  101 101 1014 1014 1014 1014 1014 1014 1014 101...  \n"
          ]
        }
      ],
      "source": [
        "# Assign the masked inputs (assumed column name \"Masked_Input\") from masked_df to pairs_df\n",
        "pairs_df[\"Masked_Input\"] = masked_df[\"Masked_Input\"]\n",
        "print(pairs_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MHv8jfPv8wg",
        "outputId": "a413d545-29d1-48ae-d703-ed15789bfd73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Drug id  1EVU  1NSI  1DJL  1AB2  1ALS  1CFG  1EG0  1OZ5  4BSJ  ...  1TVB  \\\n",
            "0  DB11300     1     0     0     0     0     1     0     0     0  ...     0   \n",
            "1  DB11311     1     0     0     0     0     0     0     0     0  ...     0   \n",
            "2  DB11571     1     0     0     0     0     1     0     0     0  ...     0   \n",
            "3  DB13151     1     0     0     0     0     1     0     0     0  ...     0   \n",
            "4  DB05383     0     1     0     0     0     0     0     0     0  ...     0   \n",
            "\n",
            "   1T5Q  6U6U  7RY7  2MDP  1G2C  4BPU  2X18  2N80  2KR6  \n",
            "0     0     0     0     0     0     0     0     0     0  \n",
            "1     0     0     0     0     0     0     0     0     0  \n",
            "2     0     0     0     0     0     0     0     0     0  \n",
            "3     0     0     0     0     0     0     0     0     0  \n",
            "4     0     0     0     0     0     0     0     0     0  \n",
            "\n",
            "[5 rows x 721 columns]\n"
          ]
        }
      ],
      "source": [
        "# --- Step 2: Load the Label Matrix ---\n",
        "# Ensure that the CSV file now has \"Drug id\" as header in the first column\n",
        "label_matrix = pd.read_csv(TARGET_LABELS_CSV)\n",
        "print(label_matrix.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {
        "id": "puiWq7vgv8wg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import ast\n",
        "\n",
        "class DtiDataset(Dataset):\n",
        "    def __init__(self, tokenized_texts, labels, max_length=128):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = ast.literal_eval(self.tokenized_texts[idx])  # Convert string to dictionary\n",
        "        item = {key: torch.tensor(val, dtype=torch.long) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure label is tensor\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woeJjeOd4TAX"
      },
      "source": [
        "#### Load BERT Model and Optimizer\n",
        "Initialize the BERT tokenizer and the `BertForSequenceClassification` model for predicting drug-target interactions (using 2 labels for single classification). Set up the AdamW optimizer with a learning rate of 2e-5 to update model weights during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdjJh56Sv8wg",
        "outputId": "5ecefde8-621d-4ed6-8461-92929f6052fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3027: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, use_auth_token=\"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\")\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6O6WePl4QVW"
      },
      "source": [
        "####Create Dataset and DataLoader\n",
        "Extract the masked sequences and map each drug ID to its corresponding binary label (derived from the label matrix). Use these to create a PyTorch DataLoader, which facilitates efficient batch processing during model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PopmsUE8v8wg",
        "outputId": "ab0334d6-f3b5-401c-a8cf-b91892a7cf7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Drug id  1EVU  1NSI  1DJL  1AB2  1ALS  1CFG  1EG0  1OZ5  4BSJ  ...  1TVB  \\\n",
            "0  DB11300     1     0     0     0     0     1     0     0     0  ...     0   \n",
            "1  DB11311     1     0     0     0     0     0     0     0     0  ...     0   \n",
            "2  DB11571     1     0     0     0     0     1     0     0     0  ...     0   \n",
            "3  DB13151     1     0     0     0     0     1     0     0     0  ...     0   \n",
            "4  DB05383     0     1     0     0     0     0     0     0     0  ...     0   \n",
            "\n",
            "   1T5Q  6U6U  7RY7  2MDP  1G2C  4BPU  2X18  2N80  2KR6  \n",
            "0     0     0     0     0     0     0     0     0     0  \n",
            "1     0     0     0     0     0     0     0     0     0  \n",
            "2     0     0     0     0     0     0     0     0     0  \n",
            "3     0     0     0     0     0     0     0     0     0  \n",
            "4     0     0     0     0     0     0     0     0     0  \n",
            "\n",
            "[5 rows x 721 columns]\n"
          ]
        }
      ],
      "source": [
        "# --- Step 5: Create Dataset and DataLoader ---\n",
        "# Extract texts from the pairs DataFrame:\n",
        "texts = pairs_df[\"Masked_Input\"].tolist()\n",
        "print(label_matrix.head())\n",
        "# For each drug id in pairs_df, get its label vector from the label_matrix.\n",
        "# Here we assume the label_matrix’s index matches the \"Drug id\" values.\n",
        "\n",
        "# Convert a multi-label vector into a single label (if that is your intended approach)\n",
        "def convert_to_binary(label_vector):\n",
        "    return 1 if any(label_vector) else 0\n",
        "\n",
        "if label_matrix.index.name != \"Drug id\":\n",
        "    label_matrix = label_matrix.set_index(\"Drug id\")\n",
        "\n",
        "labels = pairs_df[\"Drug id\"].apply(\n",
        "    lambda drug: convert_to_binary(label_matrix.loc[drug].values.tolist())\n",
        ").tolist()\n",
        "# print(label_matrix.head())\n",
        "#labels = pairs_df[\"Drug id\"].apply(lambda drug: label_matrix.loc[drug].values.tolist()).tolist()\n",
        "\n",
        "# Now create the dataset with the extracted lists\n",
        "dataset = DtiDataset(texts, labels, tokenizer)\n",
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {
        "id": "hwFKhFjlq1OT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import contextlib\n",
        "from torch.cuda import amp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged dataset labels: Counter({1: 327})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Merged dataset labels:\", Counter(labels))  # Check overall dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before splitting: Counter({1: 217, 0: 62})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Before splitting:\", Counter(processed_data[\"Interaction\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed Data Shape: (279, 2)\n",
            "Processed Data Columns: Index(['Masked_Input', 'Interaction'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load processed tokenized data\n",
        "with pd.HDFStore('final_bert_inputs_masked.h5', mode='r') as store:\n",
        "    processed_data = store['df']  # Assuming 'df' is the key used when saving\n",
        "\n",
        "print(\"Processed Data Shape:\", processed_data.shape)\n",
        "print(\"Processed Data Columns:\", processed_data.columns)\n",
        "\n",
        "# Extract tokenized inputs and labels\n",
        "tokenized_inputs = processed_data['Masked_Input'].tolist()  # Pre-tokenized inputs\n",
        "labels = processed_data['Interaction'].tolist()  # Labels (0 or 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['101 101 1017 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 19628 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 6387 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 103 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 16232 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 2184 2382 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 2538 1014 1014 1014 22318 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 6881 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 27591 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 9699 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 6796 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 102', '101 101 1017 1014 103 1014 103 1014 1014 1014 1014 1014 103 103 103 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 103 1014 6282 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 3438 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1018 2570 2603 2484 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 103 2324 2539 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 2260 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 22766 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 103 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1016 6255 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 2423 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 103 103 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 5401 1014 103 1014 17953 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1018 5388 5594 1014 1014 10767 1014 1014 1014 23145 103 1014 1014 1014 102', '101 101 6275 103 103 1014 103 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 6640 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 14155 2570 2603 2484 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2321 2324 2539 1014 103 1014 22054 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 2340 2260 1014 1014 28865 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1016 5786 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 2385 3770 1014 1014 1014 2907 1014 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 18214 1014 1014 1014 2459 4749 4583 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 3603 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 4413 103 1014 1014 103 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 13526 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1015 3429 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 13560 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 102', '101 101 1017 1014 103 103 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 17800 6275 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 22442 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 103 2184 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1017 103 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 3744 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 28510 1014 1014 1014 1014 1014 1014 1014 2423 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1017 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1850 3910 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 6275 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 103 1014 6640 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 27523 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 376 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1018 2570 2603 2484 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 102', '101 101 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 3511 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 4333 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1019 1020 4229 4464 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 441 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1021 2871 4601 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 103 1014 103 103 1014 103 1014 1014 1014 1014 1014 5013 1014 1014 1014 1014 1014 1014 1014 1014 2410 2403 4868 1014 1014 103 1014 103 1014 1014 1014 29368 103 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1023 4720 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 4724 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 103 1014 2184 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1018 2570 2603 2484 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 2321 2324 2539 1014 1014 1014 1014 103 1014 103 1014 1014 2276 1014 103 102']\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_inputs[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 223\n",
            "Test dataset size: 56\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokenized_inputs, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Train dataset size: {len(train_inputs)}\")\n",
        "print(f\"Test dataset size: {len(test_inputs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Labels Distribution: Counter({1: 173, 0: 50})\n",
            "Test Labels Distribution: Counter({1: 44, 0: 12})\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Labels Distribution:\", Counter(train_labels))\n",
        "print(\"Test Labels Distribution:\", Counter(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 542,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1    173\n",
            "0     50\n",
            "Name: count, dtype: int64\n",
            "1    44\n",
            "0    12\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(pd.Series(train_labels).value_counts())\n",
        "print(pd.Series(test_labels).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 543,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 223\n",
            "Number of testing samples: 56\n"
          ]
        }
      ],
      "source": [
        "# Create dataset instances using pre-tokenized data\n",
        "train_dataset = DtiDataset(train_inputs, train_labels)\n",
        "test_dataset = DtiDataset(test_inputs, test_labels)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 544,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Model Initialization and Optimizer\n",
        "# -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 545,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Optional: Set up Mixed Precision Training (if using GPU)\n",
        "# -------------------------\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = amp.GradScaler() if use_amp else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 546,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8AMiKluIQBnr",
        "outputId": "209bac4a-928d-4169-d3f4-c0740cc41ea7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1:   0%|          | 0/112 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<unknown>, line 1)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[0;32mIn[546], line 10\u001b[0m\n    for batch in progress_bar:\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m in \u001b[0;35m__iter__\u001b[0m\n    for obj in iterable:\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m in \u001b[0;35m__next__\u001b[0m\n    data = self._next_data()\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m in \u001b[0;35m_next_data\u001b[0m\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m in \u001b[0;35mfetch\u001b[0m\n    data = [self.dataset[idx] for idx in possibly_batched_index]\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m in \u001b[0;35m<listcomp>\u001b[0m\n    data = [self.dataset[idx] for idx in possibly_batched_index]\u001b[0m\n",
            "\u001b[0m  Cell \u001b[0;32mIn[532], line 15\u001b[0m in \u001b[0;35m__getitem__\u001b[0m\n    encoding = ast.literal_eval(self.tokenized_texts[idx])  # Convert string to dictionary\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:64\u001b[0m in \u001b[0;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
            "\u001b[1;36m  File \u001b[1;32mc:\\Users\\Neha Desai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
            "\u001b[1;36m  File \u001b[1;32m<unknown>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    101 101 1017 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 103 1014 1014 1014 1014 1014 103 1014 103 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 13367 1014 103 1014 1014 1014 1014 1014 1014 103 103 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 23457 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 5401 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 9746 1014 1014 1014 103 1014 1014 1014 18120 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 1014 1014 103 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 103 1014 1014 103 1014 26031 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 5401 1014 1014 1014 103 1014 1014 23502 1014 1014 1014 1014 1014 1014 1014 1014 1014 26539 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 103 1014 1014 103 1014 1014 103 1014 1014 1014 103 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 1014 102\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Training Loop with Progress Bar\n",
        "# -------------------------\n",
        "num_epochs = 1\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "    # Move batch to the appropriate device\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}  # Exclude 'token_type_ids'\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training if available\n",
        "        with amp.autocast() if use_amp else contextlib.nullcontext():\n",
        "            outputs = model(**batch)  # Model expects only 'input_ids' and 'attention_mask'\n",
        "            loss = outputs.loss\n",
        "\n",
        "\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} finished, average loss: {avg_loss:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation Loop with Progress Bar\n",
        "# -------------------------\n",
        "model.eval()\n",
        "correct = total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "        correct += (preds == batch['labels']).sum().item()\n",
        "        total += len(batch['labels'])\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abgzx1g9v8wh",
        "outputId": "7a7a8e20-3eb2-4e2b-f94c-e32d7817c886"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "# model.train()\n",
        "# for epoch in range(1):  # Example: 3 epochs\n",
        "#     for batch in tqdm(train_loader):\n",
        "#         outputs = model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "# print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjpwHdCm33ad"
      },
      "source": [
        "### Model Evalutation\n",
        "Due to small dataset and overfitting, accuracy is 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLs5DcS4v8wh",
        "outputId": "725c6686-4a99-49fe-f453-048c40e15ebb"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "# model.eval()\n",
        "# correct = total = 0\n",
        "# with torch.no_grad():\n",
        "#     for batch in tqdm(train_loader, desc=\"Evaluating\"):\n",
        "#         outputs = model(**batch)\n",
        "#         preds = outputs.logits.argmax(dim=1)\n",
        "#         correct += (preds == batch['labels']).sum().item()\n",
        "#         total += len(batch['labels'])\n",
        "# accuracy = correct / total\n",
        "# print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-N16coW3_vj"
      },
      "source": [
        "###Prediction Function\n",
        "Takes a drug ID and a PDB ID as inputs, constructs the BERT input sequence, tokenizes it, and uses the trained model to predict whether an interaction exists. The function returns \"yes\" or \"no\" based on the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UicD2YXWv8wh",
        "outputId": "80ee173e-4e13-4a4d-80de-09e4a570cbf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'yes'"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict_interaction(drug_id, pdb_id, tokenizer=tokenizer, model=model):\n",
        "     input_str = f\"[CLS] {drug_id} [SEP] {pdb_id} [SEP]\"\n",
        "     inputs = tokenizer(input_str, return_tensors='pt', truncation=True, max_length=512)\n",
        "     outputs = model(**inputs)\n",
        "     prediction = outputs.logits.argmax(dim=1).item()\n",
        "     return \"yes\" if prediction == 1 else \"no\"\n",
        "predict_interaction('DB11300','1NSI')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
