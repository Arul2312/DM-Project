{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MLGANN_Dataset(Dataset):\n",
    "    def __init__(self, drug_ids, target_ids, labels, adjacency_matrix, feature_matrix):\n",
    "        self.drug_ids = np.clip(drug_ids, 0, feature_matrix.shape[0] - 1)  # Fix out-of-bounds IDs\n",
    "        self.target_ids = np.clip(target_ids, 0, feature_matrix.shape[0] - 1)\n",
    "        self.labels = labels\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self.feature_matrix = feature_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drug_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        drug_id = self.drug_ids[idx]\n",
    "        target_id = self.target_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Extract drug and target features\n",
    "        drug_feature = self.feature_matrix[drug_id]\n",
    "        target_feature = self.feature_matrix[target_id]\n",
    "\n",
    "        return {\n",
    "            \"drug_id\": drug_id,\n",
    "            \"target_id\": target_id,\n",
    "            \"drug_feature\": drug_feature,\n",
    "            \"target_feature\": target_feature,\n",
    "            \"label\": label,\n",
    "            \"adjacency_matrix\": self.adjacency_matrix  # Full adjacency matrix for GNN\n",
    "        }\n",
    "\n",
    "def generate_negative_samples(AY, num_neg_samples):\n",
    "    \"\"\"\n",
    "    Generates negative samples by selecting (drug, target) pairs where AY == 0.\n",
    "    \"\"\"\n",
    "    nd, nt = AY.shape\n",
    "    neg_samples = []\n",
    "    \n",
    "    while len(neg_samples) < num_neg_samples:\n",
    "        d = np.random.randint(0, nd)  # Random drug index\n",
    "        t = np.random.randint(0, nt)  # Random target index\n",
    "        if AY[d, t] == 0:  # Ensure it's not a positive sample\n",
    "            neg_samples.append((d, t, 0))  # Label 0 for negative sample\n",
    "\n",
    "    return neg_samples\n",
    "\n",
    "def load_data(AM, AY, feature_matrix, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepares dataset for MLGANN with negative sampling.\n",
    "    - AD: Drug-drug similarity matrices\n",
    "    - AT: Target-target similarity matrices\n",
    "    - AY: Drug-target interaction matrix (binary)\n",
    "    - feature_matrix: Initial node feature matrix (drugs & targets)\n",
    "    \"\"\"\n",
    "    # Extract positive samples (confirmed interactions)\n",
    "    pos_samples = list(zip(*np.where(AY == 1)))  # [(drug_id, target_id)]\n",
    "    pos_samples = [(d, t, 1) for d, t in pos_samples]  # Label = 1\n",
    "\n",
    "    # Generate negative samples (same number as positive samples)\n",
    "    neg_samples = generate_negative_samples(AY, num_neg_samples=len(pos_samples))\n",
    "\n",
    "    # Combine positive and negative samples\n",
    "    all_samples = pos_samples + neg_samples\n",
    "    np.random.shuffle(all_samples)  # Shuffle for randomness\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    drug_ids, target_ids, labels = zip(*all_samples)\n",
    "    drug_ids, target_ids, labels = np.array(drug_ids), np.array(target_ids), np.array(labels)\n",
    "\n",
    "    # Train-test split (2/3 training, 1/3 testing)\n",
    "    drug_train, drug_test, target_train, target_test, label_train, label_test = train_test_split(\n",
    "        drug_ids, target_ids, labels, test_size=1/3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = MLGANN_Dataset(drug_train, target_train, label_train, AM, feature_matrix)\n",
    "    test_dataset = MLGANN_Dataset(drug_test, target_test, label_test, AM, feature_matrix)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.08139535, 0.04615385, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08139535, 1.        , 0.06349206, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.04615385, 0.06349206, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target_labels = pd.read_csv(\"Datasets/raw/target_labels.csv\")\n",
    "targets = pd.read_csv(\"Datasets/raw/protein_sequences.csv\")['pdb_id'].tolist()\n",
    "\n",
    "AY = target_labels.filter(items = targets).to_numpy()\n",
    "# AY\n",
    "\n",
    "AM = pd.read_csv(\"Datasets/processed/AM.csv\", index_col=0).to_numpy()\n",
    "AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (1722, 128)\n"
     ]
    }
   ],
   "source": [
    "nd, nt = 304, 405\n",
    "feature_dim = 128\n",
    "feature_matrix = np.random.rand(1722, feature_dim)  # Shape: (nd + nt, feature_dim)\n",
    "\n",
    "print(\"Feature matrix shape:\", feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(AM, AY, feature_matrix, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drug_id': tensor([ 39, 237, 145, 114,  94, 131, 297, 244, 274, 281, 115, 224, 282, 195,\n",
      "        283,  20, 253, 221, 294, 154,  17,  11, 166, 122,  51,  80, 187,  43,\n",
      "        204, 201, 281, 233, 257,  57, 284, 136, 243, 109,  82, 281, 210,  47,\n",
      "          4, 282, 146,  77,  29, 209, 179, 256, 100, 175, 252, 215, 167, 278,\n",
      "        178,  52, 108, 200, 132,  20, 209, 243,  72, 104, 153, 253, 121,   4,\n",
      "        279, 282, 227,  14, 100,  19, 165,  30,  42, 215, 303, 221, 134,  41,\n",
      "        116,  59, 165, 283, 132, 147,  40,  49, 154, 204, 222, 245, 190, 250,\n",
      "        153, 282,  59, 187,   6, 204, 103, 269, 228,   7, 293, 259,  41, 241,\n",
      "         82,  77, 287,  60, 216, 138, 186,   2,   8, 131, 115, 298,  80,  12,\n",
      "        115, 170]), 'target_id': tensor([ 17,   2, 345,  58,  86,  61,  86, 126, 123,  10,   4, 383,  54, 120,\n",
      "        196, 295,  24, 377, 122,  79,   5,   4, 177, 262,  25,  74,  72, 342,\n",
      "        389,  87, 332, 308, 316, 140, 356, 143,  99, 389,  35,  26, 365, 102,\n",
      "        139, 350,  19,  37,  13, 132, 293, 316,  83, 201, 272, 389,  81, 217,\n",
      "         35, 257,  50, 222, 304,  67, 355, 237, 150, 123, 284,  92, 286, 141,\n",
      "         28, 112, 161, 121, 289,   7, 233,   5,  74, 148, 398, 166,  62, 329,\n",
      "        358,  25, 391, 256,  63, 123, 212, 342, 159, 125, 281, 290, 218, 389,\n",
      "         79, 351, 130,  57,  83, 113, 212, 165, 284, 208,  40, 218, 100,  79,\n",
      "        116, 116, 238,  64, 199, 206, 135, 145, 165,  63,  60, 388, 141, 267,\n",
      "         93,  86]), 'drug_feature': tensor([[0.9102, 0.7560, 0.5291,  ..., 0.5439, 0.2349, 0.6591],\n",
      "        [0.7122, 0.1747, 0.1094,  ..., 0.9518, 0.9449, 0.6761],\n",
      "        [0.6257, 0.9922, 0.8392,  ..., 0.7665, 0.1513, 0.4010],\n",
      "        ...,\n",
      "        [0.6654, 0.5840, 0.1802,  ..., 0.0979, 0.9498, 0.7504],\n",
      "        [0.2932, 0.8598, 0.2287,  ..., 0.1545, 0.9133, 0.8655],\n",
      "        [0.8120, 0.9920, 0.0318,  ..., 0.7658, 0.9305, 0.5616]],\n",
      "       dtype=torch.float64), 'target_feature': tensor([[0.2530, 0.2080, 0.1943,  ..., 0.4272, 0.7220, 0.1648],\n",
      "        [0.1073, 0.0278, 0.4438,  ..., 0.3240, 0.4173, 0.3105],\n",
      "        [0.2762, 0.5125, 0.1072,  ..., 0.8127, 0.2579, 0.9323],\n",
      "        ...,\n",
      "        [0.1593, 0.8189, 0.0027,  ..., 0.3607, 0.9852, 0.1744],\n",
      "        [0.1277, 0.2697, 0.2316,  ..., 0.9860, 0.7876, 0.8290],\n",
      "        [0.8447, 0.7713, 0.4653,  ..., 0.2594, 0.6399, 0.0336]],\n",
      "       dtype=torch.float64), 'label': tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 1]), 'adjacency_matrix': tensor([[[1.0000, 0.0814, 0.0462,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0814, 1.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0462, 0.0635, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0814, 0.0462,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0814, 1.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0462, 0.0635, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0814, 0.0462,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0814, 1.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0462, 0.0635, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000, 0.0814, 0.0462,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0814, 1.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0462, 0.0635, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0814, 0.0462,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0814, 1.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0462, 0.0635, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0814, 0.0462,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0814, 1.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0462, 0.0635, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def create_pyg_graph(adjacency_matrix, feature_matrix):\n",
    "    edge_index = torch.nonzero(torch.tensor(adjacency_matrix)).t().contiguous()\n",
    "    x = torch.tensor(feature_matrix, dtype=torch.float)\n",
    "\n",
    "    # Ensure edges are within bounds\n",
    "    max_index = x.shape[0] - 1\n",
    "    edge_index = edge_index[:, (edge_index[0] <= max_index) & (edge_index[1] <= max_index)]\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Status bar for training\n",
    "\n",
    "def train_model(model, train_loader, graph, adjacency_matrix, optimizer, criterion, num_epochs=100, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    adjacency_matrix = adjacency_matrix.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            drug_ids = batch[\"drug_id\"].to(device)\n",
    "            target_ids = batch[\"target_id\"].to(device)\n",
    "            labels = batch[\"label\"].float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(graph, drug_ids, target_ids, adjacency_matrix)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / len(train_loader))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (1722, 128)\n",
      "Graph Nodes: 1722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.07it/s, loss=32.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Loss: 361.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:09<00:00,  1.16it/s, loss=46.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] - Loss: 507.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:09<00:00,  1.17it/s, loss=48.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] - Loss: 532.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:09<00:00,  1.16it/s, loss=44.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] - Loss: 493.9293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:09<00:00,  1.16it/s, loss=47.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] - Loss: 522.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:09<00:00,  1.15it/s, loss=47.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] - Loss: 521.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:09<00:00,  1.12it/s, loss=49.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] - Loss: 547.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.09it/s, loss=48.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] - Loss: 531.4048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.01it/s, loss=53.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] - Loss: 584.4793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:12<00:00,  1.12s/it, loss=52.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] - Loss: 581.9721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:12<00:00,  1.11s/it, loss=51.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] - Loss: 565.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:14<00:00,  1.29s/it, loss=52.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] - Loss: 575.7812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:14<00:00,  1.31s/it, loss=52.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] - Loss: 581.2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:13<00:00,  1.24s/it, loss=52.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] - Loss: 577.3438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:14<00:00,  1.31s/it, loss=52]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] - Loss: 572.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:13<00:00,  1.22s/it, loss=52]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] - Loss: 572.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:12<00:00,  1.16s/it, loss=51.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] - Loss: 570.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:13<00:00,  1.25s/it, loss=52.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] - Loss: 580.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:12<00:00,  1.10s/it, loss=53.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] - Loss: 592.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:12<00:00,  1.10s/it, loss=54.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] - Loss: 601.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models.mlgann import MLGANN\n",
    "from torch import nn\n",
    "\n",
    "graph = create_pyg_graph(AM, feature_matrix)\n",
    "\n",
    "# Define model, optimizer, loss function\n",
    "input_dim = feature_matrix.shape[1]  # Feature size\n",
    "hidden_dim = 256\n",
    "output_dim = 1  # Binary classification\n",
    "num_layers = 2  # Number of GCN layers\n",
    "\n",
    "# Initialize MLGANN model\n",
    "model = MLGANN(input_dim, hidden_dim, output_dim, num_layers, num_heads=4, dropout=0.2)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "\n",
    "print(\"Feature Matrix Shape:\", feature_matrix.shape)\n",
    "# print(\"Max Drug ID:\", max(drug_ids), \"Max Target ID:\", max(target_ids))\n",
    "print(\"Graph Nodes:\", graph.num_nodes)\n",
    "# Train model\n",
    "train_model(model, train_loader, graph, torch.tensor(AM, dtype=torch.float), optimizer, criterion, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, graph, adjacency_matrix, device=\"cpu\"):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    graph = graph.to(device)\n",
    "    adjacency_matrix = adjacency_matrix.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_scores = []\n",
    "\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\", leave=True)  # Add progress bar\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for batch in progress_bar:\n",
    "            drug_ids = batch[\"drug_id\"].to(device)\n",
    "            target_ids = batch[\"target_id\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # **Forward pass with adjacency matrix**\n",
    "            outputs = model(graph, drug_ids, target_ids, adjacency_matrix).squeeze()\n",
    "            scores = outputs.cpu().numpy()\n",
    "            predictions = (outputs > 0.5).long().cpu().numpy()  # Convert to binary (0/1)\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predictions)\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "            # **Update progress bar with evaluation step**\n",
    "            progress_bar.set_postfix(current_batch_accuracy=(predictions == labels).mean())\n",
    "\n",
    "    # **Compute Metrics**\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=1)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=1)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=1)\n",
    "    auc = roc_auc_score(all_labels, all_scores)\n",
    "\n",
    "    # **Print Final Evaluation Metrics**\n",
    "    print(\"\\nðŸš€ Model Evaluation Results:\")\n",
    "    print(f\"âœ… Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"âœ… Precision: {precision:.4f}\")\n",
    "    print(f\"âœ… Recall:    {recall:.4f}\")\n",
    "    print(f\"âœ… F1-score:  {f1:.4f}\")\n",
    "    print(f\"âœ… AUC-ROC:   {auc:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.70it/s, current_batch_accuracy=0.667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Model Evaluation Results:\n",
      "âœ… Accuracy:  0.4737\n",
      "âœ… Precision: 0.4321\n",
      "âœ… Recall:    0.1064\n",
      "âœ… F1-score:  0.1707\n",
      "âœ… AUC-ROC:   0.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.47368421052631576,\n",
       " 0.43209876543209874,\n",
       " 0.10638297872340426,\n",
       " 0.17073170731707316,\n",
       " 0.48063628431438354)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, graph, torch.tensor(AM, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmin\u001b[39m(all_scores))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmax\u001b[39m(all_scores))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_scores' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Min Score:\", min(all_scores))\n",
    "print(\"Max Score:\", max(all_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
